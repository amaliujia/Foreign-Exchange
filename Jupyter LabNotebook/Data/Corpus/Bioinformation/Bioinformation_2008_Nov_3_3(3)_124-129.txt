Background
The development of microarray technology has made it possible to measure the expression levels of tens of thousands of genes in 
parallel and enhance our understanding of functional genomics. An important task in DNA microarray data analysis is to identify genes 
which have similar expression patterns in order to understand their biological functions and cellular processes. This process can be 
done manually, in which case the amount of effort would be tremendous and intensive. Thus, it is important to develop computerized data 
analysis techniques, such as classification algorithms, which are needed in many applications. In our previous study, we proposed a 
sub-dimension based probabilistic neural network to solve this problem []. 
Probabilistic neural network (PNN) was first developed by D. Specht [], 
[]. It provides a general solution to pattern classification problems by using 
the Bayes strategy for probability density functions. It is frequently employed in pattern classification and microarray data clustering 
due to its prominent time efficiency. It provides a considerable improvement in training speed compared to the conventional 
back-propagation network (BPN). Furthermore, as discussed in [], PNN could attain 
the same accuracy as back-propagation neural network (BPN).

We assume that the input data consist of an n by d matrix X, where n 
is the number of genes (objects) and d the
number of conditions (features). The sub-dimension based method partitions the dataset into several smaller parts
called sub-dimensions, which may or may not be disjoint []. It clusters the 
datasets based on their sub-dimensions. In our previous study, a voting system was used to combine all sub-dimension class results. We 
assigned two objects x1 and x2 to the same group if more than half of the sub-dimensions x1j and x2j belong to the same group. Experiment 
results show that the method is effective []. However, the enormous number of 
features in the real world microarray datasets makes it difficult to select the optimal sub-dimensions. One method is to reduce the
dimensionality. In the classification, the contribution of each sub-dimension is not equal. Some may be corrupted or less relative to 
others, which can be discarded without degrading the performance of the system. In this paper, we employ the feature evaluation and 
selection technique to determine the sub-dimensions that are not as important as others in order to reduce the number of sub-dimensions
without affecting the classification accuracy.

The aim of feature selection is to discriminate features which contain the most or the least effective information
from an original candidate set. Feature selection algorithms have been well researched in this area. In our study, we
apply the entropy based measure combined with the subdimension method. Entropy based methods have been used
in many areas, such as mathematics, communication theory, and economics. In 1948, Shannon 
[] first introduced the basic entropy and the information gain concept to the 
information domain. “Entropy is a measure of the amount of uncertainty in the outcome of a random experiment, or equivalently, a measure 
of the information obtained when the outcome is observed.” [] In our study,
the entropy can be said to be the measure of contribution that a single sub-dimension makes to the general classification. Aiming to 
show the convincing performance of the proposed method, normal PNN and sub-dimension combined PNN are used in experimental comparison. 
In this paper, we first briefly review the structure of the PNN, discuss the sub-dimension formulation, and introduce the entropy 
concept. Then, we describe the proposed method and present experiment results from six datasets.

Methodology
Please see .

Discussion
Experiments based on the proposed method are performed on four microarray datasets including yeast cell cycle data, sporulation data, 
rodrigues data, and annot data [–].
To verify the proposed method, we also present the experiment results on other datasets, including wine data, Wisconsin diagnostic 
breast cancer (wdbc) data. For each dataset, we run the steps in section II 30 times and compute their average to evaluate the 
performance.

Real world data
In order to evaluate the performance of the proposed method for noisy data, we added white Gaussian noise (wgn) randomly into the 
features of entire datasets as a form of corruption. The wine dataset contains 178 objects in three groups and 13 features. In our 
experiment, we adopt 78 objects as training samples and the remaining 100 objects for testing. As shown in 
Table 1 (), the sub-dimension based PNN obtains 90 correct out of 100, 
compared with 71 correct out of 100 in normal PNN. However, with 89¢ accuracy, we can see that the proposed method provides a 
comparable performance with the sub-dimension based PNN.

The wdbc dataset has 576 objects in two classes and 30 features in which 276 training samples and 300 testing
samples are used to test the recognition results. As in the case for the wine data, the proposed method shows close
results in the wdbc dataset, 279 correct classifications compared with 280 by the sub-dimension based PNN, and
is superior to the normal PNN.

Microarray data
The yeast cell cycle dataset consisting of 6220 genes is published by Cho and colleagues []. 
In the study of the sub-dimension method [], we adopt 384 genes and normalized 
each gene expression profile so that it has zero mean and unit variance. The dataset has five cycle phases which are the G1 phase, late 
G1 phase, S phase, S2 phase and M phase, and 17 time points. The results are given in Table 3 under 
. The proposed method correctly classifies 149 out of 
200 testing samples and the sub-dimension based PNN correctly classifies 150. The error is only 0.5¢.

The sporulation dataset contains 6118 genes with seven features. In [], after 
pre-processing, we use only 1136 genes of which the value of the root mean square of the log2 transformed the data greater than 1.13. 
The dataset has seven phases: metabolic, early I, early II early middle, middle, mid-late, and late. We use 736 genes for training
and the remaining 400 genes for testing. As shown in Table 4(), the 
proposed method works well with an accuracy rate of 48.5¢ (194 out of 400) compared with 49.5¢ for the sub-dimension based PNN.

Rodriguez dataset is available elsewhere []. It contains 974 genes clustered 
to nine groups with 47 features and 500 of the genes are used for testing. Clearly Table 5 
() shows that the proposed method achieves an improvement of the same recognition accuracy with the sub-dimension 
based PNN (82.4¢). As comparison, the normal PNN classification results are 79.6¢ accuracy. Similar results on the 
Annton dataset, containing 639 genes in five classes and 47 features, of which half are in the test set. As expected, the test set
presents almost the same success as the sub-dimension based PNN, at 73¢ accuracy. The normal PNN could only obtain 283 correct out of 
400 testing data. As shown in the tables (under ), the 
proposed method performs very closely to the sub-dimension based PNN which uses all sub-dimension features.

Conclusion
Instead of considering all features of datasets in a classifier, our previous paper [] 
implemented the PNN classification on single sub-dimensions. However, the number of combinations of sub-dimensions is large and this overall
system computationally to complicated. In this paper, a feature evaluation and selection technique based on an entropy definition is 
used to measure the contribution of each sub-dimension. The sub-dimension with the lowest contribution to the overall classification is 
discarded. Experiments on two real world datasets and four microarray datasets show clearly that the achievement of the proposed 
technique is remarkable better than the normal PNN and as good as the sub-dimension based PNN. However the system complexity is 
significantly reduced and the classification speed is increased. The feature evaluation and selection are especially effective and
convenient when the input features are large and the datasets are noisy. At the rank of the corresponding information gain G, the 
importance of the sub-dimension decreases while G reduces. Good performance selection occurs particularly at the top of the rank. 
However, how many sub-dimensions should be considered as important is a critical issue which needs to be investigated further.

Supplementary material
Data 1
