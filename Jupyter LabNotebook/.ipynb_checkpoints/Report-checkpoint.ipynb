{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statements\n",
    "The problem needs to slove here is, given a bunch of unlabeled documents, and given a word, how to find semantic related words?\n",
    "\n",
    "Why this problem is important? In order to solve this problem, a serial of subproblems need to be solved too, like topic detection, semantic analysis, etc. All these problems are crital to speech and text processing, in other words, they are critial for computers to understand human's languages.\n",
    "\n",
    "###\tDesign experiments to test these hypotheses\n",
    "*   Collect the corpus. (Documented as text files)\n",
    "    I use PubMed bio-documents as my corpus. Here is the [link](ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/) to their website.\n",
    "    \n",
    "    \n",
    "*   Select Model\n",
    "    The algorithms and models are one of the most important parts for sloving this problem. As far as I know, there are two ways to solve this problem. \n",
    "    1. Ranking words by repeated co-occurrence. The co-occurrence is mearsured by tf-idf, that is for the words that apprear along with target word in documents, then words with higher td-idf score are ranked higher.\n",
    "    <img src=\"images/f1.png\" style=\"border:none;\"> <br>\n",
    "    2. Ranking words based on Conditional Probability. Here is the formula:\n",
    "    <img src=\"images/f2.png\" style=\"border:none;\"> where z is a topic, Z is the topic collection, Wt is target word, Dt is the collection of the document. <br> \n",
    "    \n",
    "    \n",
    "    Eventually I chose the second method. The second topic based model is a better way to who the semantic relationship among words. The documents themselves are reprented by topic distributions. Then the occurrence probability of word in document is also conditional determined by topic distribution. Topic is actually a random variable of words occurrence, in others words, it is a vocabulary with word specific probability. So topic is a good way to model semantic. The first idea just ingore the semantice features.\n",
    "    \n",
    "    \n",
    "*  Feature extraction\n",
    "    1. I use MALLET libary, which is a java implementation of Latent Dirichlet allocation topic model, to compute topic distribution of documents.\n",
    "    2. Base on topic model, word-topic counts are computed.\n",
    "    3. The platform that runs my feature extraction code, has two, four core Xeon E5345 processor (2.33GHz, 8M L2 cache, no hyper-threading), 16GB memory and hard disk(15000RPM).\n",
    "    \n",
    "\n",
    "* Metrics<br>\n",
    "After careful consideration, eventually I dicided not use metrics to measure the result. There are a few reasons why I make such decision. The first reason is there is no obvious baseline approach. Randomly picking up words is not a good baseline. Of course I can choose another way as the reference but it is hard to tell which one is better, then it is hard to say which one should be baseline. The next reason is I believe evaluatation can only be based on specialists. So human labels and evaluation is better.\n",
    "\n",
    "\n",
    "* Data Visualization<br>\n",
    "I use D3 library to visualize my data. D3 is a javascript library with lots of beautiful visual features that power data science. I use one of the templates.\n",
    "\n",
    "  \n",
    "###\tRecord your experiments and the exact parameters/methods used\n",
    "The main model I used is LDA topic detection. MALLET is a java library that implements LDA. There are two parameters:\n",
    "<ol type=\"a\">\n",
    "  <li>The number of topic to detect. Currenct model is kind of unsupervised learning. So there is no labels on corpus. Then how many topics in corpus is a big problem. Less topics are easy to compute, but may cannot distinguish words. More topics are computational expensive, but may help me fetch higher semantic related words. I use 20 topics.</li>\n",
    "  <li>The number of iteration for parameters updating. For LDA model, usually 1000 to 2000 interations are preferred. I use 1000 iterations here.</li>\n",
    "  <li>The number of desired words. For given words, I fetch top 10 words from semantic similarity ranking list. For each word out of this fetched 10 words, I fetch 3 words, so total 30 words form up the second level list.</li>\n",
    "</ol>\n",
    "###\tRecord the results of your experiments\n",
    "Here is words graph for \"morphology\".\n",
    "<img src=\"images/f3.png\" style=\"border:none;\">\n",
    "\n",
    "###\tInterpret and connect the pattern of experiment results\n",
    "The top 10 semantic similar words of \"morphology\" is \"cells\", \"cell\", \"protein\", \"al\", \"data\", \"expression\", \"analysis\", \"species\", \"result\", \"study\". These 10 words looks highly correlated. Morphology is a biology subject, and \"cells\", \"protein\" are study targets in this subject. In order to study problems, occurence of \"data\", \"study\", \"analysis\", \"result\" make lots of sense. Finally, \"expression\", \"species\" are also related to \"morphology\".\n",
    "\n",
    "Overall speaking, the first level words are great.\n",
    "\n",
    "The for each level one word, I tried to fetch top 4 semantic similar words. As a example, \"results\" has \"data\", \"protein\", \"cells\", \"result\". Such list looks good, excpet \"results\" itself appears in this list. However, looking at another list would tell a problem. For \"expression\", the similar words list contains \"data\", \"cells\", \"protein\", \"gene\". As you can see, the problem is, there is overlapping in word lists in second level. So the effect of word discovery in level two is not good. What I excepted was level two could give me more unseen but semantic correlated words.\n",
    "\n",
    "To conclude, my solution works well in first level. This shows the power of LDA and the effectiveness of topic representation for words. The idea that represents words as a distribution of topics is very effective to compute the sematic similarity and such similarity can show the essence of words.\n",
    "\n",
    "###\tError Analysis\n",
    "So how to solve the inaccuracy issue in second levels.\n",
    "\n",
    "One straight idea, is first building a vocabulary that collects all the words that are seen so far. Then in the second(following) levels, skip the words that has been collected in this vocabulary. At least this idea can help avoid repeated words.\n",
    "\n",
    "The next idea, is if I can tune the number of detected topics. The number of topics affect the representation of the words. I only used 20 topics setting, which might be so less that cannot distinguish words from semantic perspective.\n",
    "\n",
    "\n",
    "Another idea, is if I can combine the currect LDA approach with co-repeated approach. Such combination may give me more options when I cannot create high qulified word lists.\n",
    "\n",
    "###\tActionable information obtained and insights\n",
    "One more interesting problem I am thinking about is, what kind of applications on which I can apply this project.\n",
    "\n",
    "Search query recommendation in Search Engines is a good example. Sometimes search engines like Google and Bing prompt words they guess that repreent users' informational needs. So such idea can implemented based on idea in this project. \n",
    "\n",
    "###\t Future work\n",
    "\n",
    "* Work on multi level words list and improve the result.\n",
    "* Work on applications on wich apply this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
