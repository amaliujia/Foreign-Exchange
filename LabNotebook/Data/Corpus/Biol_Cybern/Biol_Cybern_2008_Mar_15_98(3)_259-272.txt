1 Introduction
Control tasks for animals or machines (systems) require sensible actions that follow from the current state of the system. As consequence of an action the state of the system may change and a new action will be elicited, and so on. This procedure describes a sequence of states and actions, which follow each other in time. Such control mechanisms can either be hardwired into the system, but many times it is more appropriate to design a learning algorithm that tries to infer the next action from the previous sequence of states. Especially in animal (or robot-) control the complexity of the world may prevent hardwiring and learning is required instead to assure enough flexibility.

In general three mechanisms can be used for learning in such situations: (1) Unsupervised learning (finding statistical structure), (2) reinforcement learning (the learner receives a fairly unspecific reward-signal about the success of its actions) and (3) supervised learning (the learner receives a specific error signal). The distinction between these algorithmic classes is to some degree fuzzy and we will here deal with classes one and two only.

Specifically there are: reward-based reinforcement learning (;  abbreviated with RL) as well as correlation based differential Hebbian learning (; ; ), here abbreviated with CL (correlation-based learning). These algorithms differ fundamentally in their design and goal.

Most influential up to date is reinforcement learning and substantial efforts have been undertaken during the last 15 years to develop a highly successful theory of RL (; ; ; ; ). Mostly RL methods had been developed in conjunction with machine learning (e.g., Q-learning, ; ) and only fewer attempts exist to design architectures which want to be more compatible with (biological) neural networks (, ; , , ; ; ; ). When studying machine learning, it becomes clear that there is no easy way to adopt the state-action space structures used for (e.g.) Q-learning to a neural network. Most often Actor-Critic Architectures (; ) are used which emulate some aspects of the basal ganglia and the prefrontal cortex. Other biologically relevant approaches have implemented TD-learning in the context of place field guided navigation models in the hippocampus (; ).

Correlation based differential Hebbian learning has been invented around 1985 (; , ) after so-called stimulus substitution models of classical (and instrumental) conditioning had been introduced (). At that time these models had not been used in any behaving closed loop system and the early CL-models had soon been superceded by the highly influential method of “temporal difference learning” (TD-learning; ) by which CL-methods essentially died out and became replaced by RL-approaches to which TD-learning belongs. Through TD (and related methods) it became possible to reliably learn the values of states and actions and to control the learning of an agent through reinforcement signals. Only much later CL was revived when discovering a method of how to embed CL into closed-loop control systems guaranteeing convergence of the learning process (; ). By this CL became a possible alternative to RL. Furthermore, it has been found (; ) that differential Hebbian learning is related to spike-timing dependent plasticity (STDP; ; ).

Hence currently one finds the following situation: TD-learning is compatible to neuronal circuitry mostly at the network level, while differential hebbian learning is more directly compatible to STDP at single neurons. Only very recently several attempts have been presented in the literature to show that there is a more direct equivalence existing between these two worlds and two different proofs have been presented how RL can be emulated by a CL formalism (; ; ; ). Alas, both approaches are quite complex and the relation between RL and CL is not really straight forward.

Therefore, it may make sense to step back and first provide some simple circuits by which RL and CL can be emulated at a single unit in a time-continuous and causal way from where on it might become easier to address the question to what degree both approaches are similar. In addition, the growing number of different learning rules, especially for CL, makes it useful to provide a comparison, which may help the reader to better understand these rules when wanting to implement them.

Note, this paper deals with the different learning rules only in an open-loop context. Behavioral feedback is not treated. This would represent a second step and is beyond the scope of the current paper.

2 Basic approaches
In general we have to deal with situations where information arrives at an agent spread out over time. Hence all methods for RL and CL need to provide some kind of memory mechanism. This can be explained in the most basic way when discussing classical conditioning models (Fig. ). In order to learn reacting to the earlier stimulus (CS), it has to be remembered in the system. To this end the concept of eligibility traces had been introduced (, ; , ; ; ), where the synapses belonging to the earlier stimulus remain eligible for modification for some time until this trace fades. In the following we will use x as the signal input and u as the filtered input (u = x * h) with h being an arbitrary filter function (we will define a particular filter function later). The * denotes a convolution. Furthermore, gu will be the neuronal sum of the inputs weighted by the weights ω and prime will denote the derivative with respect to time: υ′ = dυ/dt.
Simple correlative temporal learning mechanism applying an eligibility trace (E-trace) at input x1 to assure signal overlap at the moment when x0 occurs; ⊗ denotes correlation. In the terminology of conditioning: CS conditioned stimulus, US unconditioned stimulus. The ω depict the synaptic weights



The first model to make use of this had been designed by . In this model synaptic weights change according to , where they have introduced two eligibility traces, u1 at the input and  at the output, given by , with control parameters ã and ~b. Mainly they discuss the case of ~b = 0 where , which turns their rule into (Fig. ): S&amp;B:  Before learning this neuron will only respond to the US, while after learning it will respond to the CS as well.
Comparison of basic learning rules. Eligibility traces are denoted as E, while x0 (or r respectively) represents the unconditioned input or reward and x1 the conditioned input. The amplifier symbol denotes the changing synaptic weight. All models use the derivative of the postsynaptic signal in order to control the weight change. Both Sutton and Barto models a, b use eligibility traces only in the learning pathway, whereas ISO Learning c and TD-Rephrased d introduce them also in the other pathway. The rephrased TD model d is mixture of models b and c. We introduce a multiplicative factor α to control the impact of u0 on the δ-value. Setting α → 0 we reobtain ISO Learning, setting ω0 = 0 we get TD Learning (with a filtered x0 path instead of an unfiltered)



TD-learning had been developed a few years later (). One centrally new aspect at that point had been to introduce a reinforcement signal r, which affects the learning, but not the output of the system (Fig. ). Synaptic weights change according to . We define:  the δ-error of TD-learning, which is the mismatch between predicted (expected) and actual reward. The parameter γ is called the discount factor, which accounts for the fact that distant rewards should be valued less. For practical purposes it makes sense to also introduce a learning rate factor μ &lt;&lt; 1 in this equation, which allows controlling the speed of learning. Hence we have: TD: 

Figure  and  shows the difference between the old S&amp;B model and TD-learning. Note, in TD the reward does not enter into the output of the neuron, but only influences the learning and both diagrams are identical if we remove x0 or r, respectively. Originally these models had been designed with the time-scale of classical or instrumental conditioning in mind, which can be seconds between CS and US. If dealing with biological systems, ultimately, however, one must match all learning models to the time scales of synaptic plasticity. When doing this, it does not make sense to separate the inputs to a neuron from their eligibility traces. Rather it seems more reasonable to assume that spikes get transformed at a membrane into several much slower processes, some of which determine the membrane potential, while others may determine plasticity. For simplicity here we might just assume one slow process at the input to a neuron which equally affects the neuron's output and its learning. When doing this we arrive at diagram Fig. , which is called ISO-learning (). The ISO-learning rule is given by ISO:  Hence, ISO-learning is a form of differential Hebbian learning. While the main difference between the S&amp;B and ISO rule is only that both x1 and x0 are filtered before they enter the neuronal summation, the ISO-learning rule behaves much different from the S&amp;B model.

If we give up the notion of an independent reward input and, like in the old S&amp;B model, use x0 also as a reward, we obtain Fig.  for rephrased TD-learning, where the learning rule takes the following shape: TD-r:  Two observations can be made for this rule:
At the beginning of learning the output υ is dominated by x0. Hence, in this formalism, rephrased TD-learning shines up as a combination of Hebbian learning with the term u0(t)u1 ≈ υ(t)u1 and differential-Hebbian learning with the term υ′(t)u1.

Also we find that this rule is (apart from a minus sign) identical to the ISO-learning rule when we set x0 = 0, a case which is of great importance in the discussion on the different convergence conditions below.



3 Stability analysis for x0, r = 0
In the following simulations sequences of two delta-pulses have been repetitively presented to the different systems, where x1 is earlier than x0 (or than r, respectively) with a constant interval T = 30 steps between them. The interval between pulse pairs was 300. The step by step weight change Δω is calculated by integrating the respective learning rule: . From this the development of the weights can be plotted. Learning rates μ have often been adjusted to yield similarly strong weight growth for the different systems when wanting to compare rules.

In addition we will show the different weight change curves plotting the weight change against the interval between inputs T. Note, strictly these curves reflect weight changes only for t → 0, hence when w1 is still approximate zero. For negative T the temporal order of the pulses is inverted.

The next figures (up to Fig. ) always show how the different learning rules behave when maximally two eligibility traces are being used. This case applies only if one has good knowledge about the temporal difference T between the two incoming stimuli. Often, however, knowledge about T is limited, then one needs to use a set of eligibility traces for spreading out the earlier stimulus across time to make sure that at least some of these signals can be related to the later occurring x0 or r signal. This situation is discussed later (see filter bank approach in Fig. ).
Architecture and open loop simulations of ISO3-Learning. Different from ISO Learning, a contains a third factor R (relevance signal), which controls the moment of the weight change. b Shows the signal structure and gives a graphical indication of the strengths of the auto-(AC) and cross-correlation (CC) contributions. c compares the weight development of ISO with ISO3 for the signal structure and filter characteristics shown in b. ISO3 does not produce any upwards drift anymore when switching x0 off. d Shows the weight change curve. The parameters of the band-bass used for R were a = 0.6, b = 0.66, σ = 0.06 and TR = T in all cases

Filter bank approaches for CL and RL. a Schematic diagram of a filter bank implementation of ISO-learning. b Signal structure for signals u. c Serial compound representation: the simulated stimulus x(t) is split into traces u11…n(t) with only one non-zero value in time, which cover the duration until the reward is presented. Each of these traces has its own weight ω11…in which gets updated after each time step according to the delta error δ. d Using TD-learning the output is zero before learning, and thus the delta error δ peaks at the same time step the reward occurs. e After learning δ has moved forward in front of the reward, while the output υ arises with the stimulus and ends with the reward (; )



As eligibility traces E we use in all cases a band-pass filter response defined by  with Θ(t) being the Heaviside (step) function. Actual parameters are given in the figure legends.

We are especially interested in the stability of the learning rules. All rules learn by cross-correlating two signals with each other (x1 with x0 or with r), correlations of x1 with itself (auto-correlations) are normally unwanted. Hence it is of interest to subdivide the contributions of the learning rule into a cross- and an auto-correlation term by: , the latter term drives the weight change of ω1 during the occurrence of x0 (or r), whereas the autocorrelation term also changes the weight in the absence of the x0 signal. Hence, the pure auto-correlation contribution becomes visible when switching x0 (or r) off.

This is what we do in the following diagrams by setting x0 = 0 at time-step t = 6000 to show if the weight change for a given rule will then indeed stop. This is an important case in a closed-loop system which, for instance, has to avoid a reflex triggered by the x0 signal (; ; ).

Let us now calculate the auto- and cross-correlation contributions for the rules introduced in the previous section.

Equation , from the S&amp;B 1981 model, leads to:  where we have assumed and will assume for all upcoming calculations a quasi-static approach (ω′ &lt;&lt; 1), exchanged u1(t) with h(t) and x1(t) and x0(t) with δ(t) and δ(t − T), respectively. Additionally we used  ().

Thus, we get: S&amp;B: 

The TD-learning rule (Eq. ) leads to similar results: TD:  The only difference is a changed sign and the derivative of the filter h associated with the x0 or rather r input (which is set to ω0δ(T) for comparison).

In general one finds that in all cases synapses ω1 will grow (left panels) until the later pulse is switched off. In the S&amp;B model (Fig. ) one sees that the unfiltered input x0 and its derivative lead to strong, needle-like excursions of the weight growth for every step, which let the line in the diagram appear broad. These disappear as soon as x0 is switched off. In TD-learning (Fig. ) according to , weights grow about ten times faster, which is due to the fact that the reward brings in only positive contributions. When we switch x0 (or r) off, then we find in both cases that ω1 drops slightly. Note, this is not what ought to be done in a TD-rule. Switching off δ would be the correct condition and obviously weights will be—by construction—stable then. Still, we want to look at the r = 0 case, because it directly corresponds to the x0 = 0 condition of the CL-rules and shows the behavior of the pure auto-correlation contribution.
Quantitative results from the architectures shown in Fig. . The left panels show weight growth of ω1 during learning with two different learning rates μ = 0.001 and μ = 0.002. The parameters of the band-pass were a = 0.3, b = 0.33 and σ = 0.03. The right panels show the relative weight change when learning starts (ω1 = 0) for different temporal intervals T between x0 and x1. A positive value of T stands for  and vice versa. a, b Model of , c, d TD-learning (). e-h ISO-learning (). f, g show the temporal development of the weight change for a single input pulse pair. f Comparison of the AC and the combined AC + CC term. g Comparison of the AC term on a magnified scale for two different sampling rates (1x, 10x) to show the numerical accuracy. i, k TD-Rephrased, where k is plotted for different values of α and the inset shows an enlargement around the origin



Weight drop of the S&amp;B- and TD-rule is due to the fact that the remaining total influence on the weight growth, which now comes only from the auto-correlation term is negative. As mentioned above, for x0 = r = 0, both diagrams (Fig. ,) are identical (see Eqs. , ) and the remaining differences in weight drop come from the different values of ω1, reached at switch-off time.

The weight change of a single signal pair for the ISO-rule (Eq. ) can be written as: ISO:  where the auto-correlation term converges to zero for t → ∞ as the filter function h(t) possesses only one maximum and eventually decays to zero.

Additionally, we calculate the time development of the cross-correlation part to give an insight into the exact weight-change: 

Panels f and g of Fig.  show the relaxation behavior of Δω for a single input pulse pair. In Fig.  a small early auto-correlation component (AC) is followed by a big, cross-correlation dominated hump (AC + CC) as soon as x0 occurs. The curve relaxes to the final weight value only after some time depending on the filter characteristic of h. In Fig.  we magnify the auto-correlation component for a situation where we have switched x0 off (auto-correlation only!). The dashed curve shows that, following Eq. , the auto-correlation indeed drops to zero after a short time. This curve was numerically calculated with a ten-fold increased sampling rate as compared to the solid-line curve shown next to it. This represents the auto-correlation contribution, when using coarser sampling and here we see a potentially very strong source of error: The auto-correlation contribution does not vanish anymore. This is a pure numerical artifact of the integration procedure, but—as high sampling rates are often too costly (for example in real-time applications)—this artifact can strongly interfere with the convergence of ISO. Hence, we are facing two potential sources of error: (1) The tardy relaxation behavior of (essentially) the cross-correlation term (Fig. ). This error becomes relevant when pulse pairs follow each other in time too quickly. And (2) the non-negligible numerical error that renders the auto-correlation non zero even for long relaxation times. The other CL-rules discussed below have been invented to solve these problems.

Figure  shows the step-by-step behavior of ISO-learning. Weight growth is similar to that of the TD-rule. Here we find that, after switching off x0, weights will drift upwards. This effect comes from the afore discussed numerical error and vanishes for very small integration step sizes Δt and large relaxation times t as shown by the dashed lines.

The rephrased TD-rule (Fig. , Eq. ) shows a ten-fold increase of the weight growth. This is due to the additional influence of u0, which now also enters the learning rule. The contributions are as follows: TD-r:  The auto-correlation term vanishes due to the same arguments as for ISO-learning. The cross-correlation term is extended by a Hebbian contribution, which causes the 10-fold increase. The antiderivative of the filter function h(t) is depicted as H(t). Otherwise, its behavior is very similar to that of the ISO-rule. As noted above, switching x0 off, transforms TD-Rephrased into ISO-learning.

When looking at the weight-change curves (Fig. ,,,), we observe that the S&amp;B model produces strong negative weights for values of T ≈ 0 (Fig. ). This effect had already been reported in their original papers ().

TD-learning produces an asymmetrical weight change curve (Fig. ). This is a consequence of the missing E-trace in the reward pathway. If being early, the reward has already vanished before x1 occurs and the correlation result remains zero.

The weight change curve of ISO-learning is anti-symmetrical (). As long as both E-traces are the same, this curve will have identical lobes on both sides (Fig. ). This is interesting, because with this rule a completely isotropic setup can be designed, in which both synapses are allowed to change as will be discussed in Fig. .
Symmetrical architectures of ISO a–d and ICO e, f where both weights ω1 and ω0 change. Learning rate was μ = 10−4. Bandpass parameters were a = 0.006, b = 0.0066 and σ = 0.006. Panel b shows the oscillating development of both weights in ISO learning starting with ω0 = ω1 = 0.01 and a temporal difference of T = 18. The spiral shaped inset indicates that phase is constant. Panel c (T = 18) and d (T = 5) depict how the choice of T influences symmetry. In contrast to ISO learning, weights in ICO Learning e are symmetrical independent of the parameters used f



The rephrased TD-rule can produce mixed properties (Fig. ). As such it will early during learning, as long as ω ≈ 0, produce essentially plain heterosynaptic Hebbian learning. This is due to the fact that the term u0, which enters the rule, has a much stronger influence than the derivative (as discussed above), which can be neglected. Only if we introduce an artificial attenuation factor α, by which we weaken the u0-input, we will receive a transition towards differential Hebbian learning, which will become identical to ISO-learning for α = 0. The inset shows magnifications of the curves. This also confirms that the plain, filtered input has a much stronger influence on the learning amplitude than the derivative.

3.1 Conditions of convergence
When do all these different algorithms converge? Trivially, weight growth at ω1 will stop as soon as x1 = 0 in all cases. Also we find that for the CL-rules weights will converge if T = 0. Hence these systems will be essentially stable if small positive values of T are followed by small negative ones (or vice versa). Other possible convergence conditions, however, are less obvious.

For the old S&amp;B model at that time convergence properties had not been investigated. Clearly Fig.  shows that for υ′ = 0 this model will converge. But this condition is not useful as it requires the output to be constant. Hence, strictly for the old S&amp;B model convergence cannot be guaranteed. On the other hand, ISO-learning is strictly convergent for x0 = 0 and weight ω1 will then stop to change. When switching x0 off (Fig. ), one can see that ω1 is (almost) stable for small learning rates (μ = 0.001). For a higher learning rate, weights drift upwards. This is the above mentioned consequence of the high numerical sensitivity of ISO learning to the integration step size.

For TD-learning there exist two different types of convergence. The first one holds for constant learning rates μ and was proven by Sutton (; ). Here, only the expected value of the output  (where tc denotes the time convergence has been achieved) converges to an optimal value. In order to ensure convergence of the output υ itself, the learning rate has to decrease over time μ(t) ∼ f (t) with the condition  and  [e.g. f (t) = 1/t); ]. Since we use time independent learning rates in this article, only the mean of the output should be constant at the end of learning. Thus both types of convergence do not need a vanishing delta error δ, instead just its average over time has to be zero: . The delta error δ itself oscillates without any reward r around zero. Simply due to the shape of the band-pass the weight is decreasing over time and does not stabilize as mentioned before. Consequently the convergence condition for TD-learning with a constant learning rate μ is  or 

Note, for this condition, the output υ, or rather its derivative, needs to take on a certain value as opposed to ISO-learning, where the input x0 needs to become zero. Hence, we have one algorithm (TD-learning) where convergence is guaranteed by output-control as opposed to one other algorithm (ISO-learning), which uses input-control to guarantee convergence. Looking back at Fig. , it becomes clear that setting r = 0 does not enforce convergence. This had been only done for auto-correlation term evaluation (comparison to the ISO-approaches).

As discussed earlier, the rephrased TD-rule shown in Fig.  carries properties of both algorithmic classes, convergence can be achieved for 〈δ〉 = 0 but also with x0 = 0, because in this case the learning circuit is identical to the ISO-learning circuit, as already mentioned above.

Furthermore, we note that there is no generic way to rephrase the TD-specific convergence criterion 〈δ〉 = 0 into an input condition. In order to attempt this, we need to use input terms only. Let us again use TD-Rephrased to show this and define x0 as the reward signal (Fig. ). Then we have . Using this in Eq.  we get as convergence condition for the synapse ω1:  Here we note that the synapse ω1, which we are supposed to stabilize, shines up on the right side. Hence, phrased in this way as an input condition, it cannot be fulfilled.

4 Features and problems of the basic architectures
Several problems exist with the above described basic approaches, most notably:
CL-rules are theoretically stable for x0 = 0, but ISO learning is highly sensitive to numerical errors which can easily destroy convergence.

Input x0 is connected to the output υ in the CL-rules. Thus, CL-rules will always produce a (motor) output which can be used to generate actions.

By construction, TD-learning is stable for 〈δ〉 = 0.

As we are dealing with a single neuron with only two inputs one finds that, without additional assumptions, TD-learning cannot produce actions: The original TD-learning as depicted in Fig.  does not produce any output at all if starting with ω1 = 0. Also, setting ω1 ≠ 0 is nonsense from a conceptional point of view. This would assume that Pawlov's dog has already some knowledge about the meaning of the bell even before the first learning experience. This, however, can be a desired aspect of TD learning as it has led to the situation that TD-learning, when considered neuronally, has only been used to judge the quality of but not to actually generate actions. Actor and Critic generically remain separate in architectures that use TD-learning to implement RL (). Other approaches are using additional inputs to produce a (motor) output signal (). While this works, we are here concerned with a more puristic view of only using two inputs unequivocally separating Actor and Critic.



5 Modified approaches
In order to address the above stated problems it is possible to modify the existing approaches.

First we ask, can we enforce stability in CL for x0 = 0 without this unwanted numerical instability? For this we need to design a learning rule for which the auto-correlation term truly vanishes.

Figure  shows an architecture, where we have replaced the derivative of the output in ISO-learning, with the derivative of the (later) input x0. Hence we are only correlating inputs with each other, thus, the name of this rule: Input correlation learning (ICO, ). The learning rule is given by ICO:  The overall weight change is similar to the ISO-rule and yields: ICO:  Here, the auto-correlation term is by definition equal to zero.

The corresponding results are shown in Fig.  and . The learning window is identical to that of the ISO-rule (Fig. ), but now weights are stable for x0 = 0. The inset in Fig.  shows the relaxation behavior for a single pulse-pair. In comparison to ISO learning (inset in Fig ) the shallow initial rising phase in missing here as there is no auto-correlation contribution. For the same reason also the “hump” is a little bit smaller. This effect, however, is barely visible even when overlaying the curves. Incidentally, ICO is identical to ISO in the limit of μ → 0. The ICO-rule has proven to be very useful in difficult learning tasks (). In fact this rule reliably works even with very high learning rates and will always converge if one manages to bring x0 down to zero. One should, however, notice that ICO-learning is a form of non-Hebbian (heterosynaptic) plasticity, which may be less realistic from a biological point of view.
Comparing the architectures shown in Fig. . a,b ICO-learning d-f Acting TD. Switch-off conditions (x0 = 0 or δ = 0) as denoted in a, c, d, f. Learning rate was μ = 0.001 in d and f. The parameters of the band-pass were set to a = 0.3, b = 0.33, and σ = 0.03, throughout. The weight change for TD-Rephrased d is plotted for different values of α and the inset shows an enlargement around the origin

Comparison of two modified learning algorithms. a ICO-learning. b Acting-TD. While Acting-TD and all architectures shown in Fig.  use the derivative of the postsynaptic signal, ICO-learning a uses the derivative of the unconditioned signal in order to control weight change. Note, x0 in Acting-TD is identical to the delta error δ



The second question concerns the problem to design a TD-learner that will also produce some motor output when learning starts (hence, when ω1 is still zero).

The rephrased TD-rule in Fig.  will indeed do this. The output will be driven by x0 early during learning and this could be used to induce behavior. Remember, this rule has two ways to enforce stability: Either 〈δ〉 = 0 or x0 = 0, which makes this rule a strange chimera between TD and ISO, also rendering it sensitive to the numerical auto-correlation problems. The question, thus, is can we find a rule that needs to enforce 〈δ〉 = 0 only and still can produce some output at the start of learning? Figure  shows one novel possible such architecture called Acting-TD. Here we feed the δ-error into the x0 input line. Note, this system achieves stability only for 〈δ〉 = 0 and will always produce some output υ.

The learning rule reads as follows: TD-a:  where υ is recursively defined as  Therefore the overall weight change can only be written as an integro-differential equation (not shown), which cannot be solved.

Figure – show how this system behaves in the open-loop condition. As expected Acting-TD is stable for 〈δ〉 = 0 (panel c). Pure Hebbian learning is implemented by this rule (panel e), even when attenuating the reward input with factor α. The feedback of δ into the input of the neuron, however, leads to a potential destabilization. To show this we have introduced the amplification factor β into the derivative of the output υ′. If this pathway enters the neuron with β larger than one (β &gt; 1), destabilization occurs as shown in Figs. . For values of β ≦ 1, the circuit, however, remains stable. Figure.  shows what happens when we invert the x0 pathway, by setting its synaptic weight ω0 to negative values. This leads to an unstable situation during weight growth and to a strong drop as soon as 〈δ〉 = 0.

Does this circuit produce reasonable behavioral output? At least we can make some statements about the shape of the output at the start and the end of learning. At the start we have ω1 = 0 and we receive , which makes sense. When the system has converged (〈δ〉 = 0) we get υ = ω1u1, which essentially amounts to a process of stimulus substitution as required from such models. The 〈δ〉 = 0 condition can also be written (though disregarding the averaging 〈〉) as . Hence we find that, if converged,  or, more specifically,  should be fulfilled.

6 Symmetrical learning rules
So far we were able to address the problem of divergence, which was present in the basic CL-rules (Fig. ) having achieved stable temporal sequence learning when employing the ICO-rule. Next we would like to ask if it is possible to implement learning (LTP) at one synapse and unlearning (LTD) at the other synapse at the same time. In principle this should be possible because one synapse experiences +T while the other experiences −T for any given input pair. Thus, causality is inverted for the two synapses and with the right design the one should grow, while the other would shrink.

Clearly TD-learning is designed in an asymmetrical way and cannot easily be symmetrized. This is different for the CL rules.

Figure.  and  show two isotropic setups. The one (Fig. ) for ISO-learning (which gave this rule its name) and the other for ICO-learning.

The learning rule for the coupled ISO-learning case is ISO-sym:  Unfortunately, this system of differential equations cannot be solved analytically.

In the case of ICO-learning the rules write as follows: ICO-sym:  Here we can solve the weight change analytically to ICO sym: 

For ISO-learning, naively one would expect that with the same starting weights (ω0 = ω1) one should get exactly anti-symmetrical learning, because the positive influence of +T at one synapse should find its exact counterpart at the other synapse which experiences −T. At least the learning curve in Fig. , seems to suggest this. However, this is in general not the case due to the fact that the overlap of the filter function is not symmetrical relative to the pulse pair. Hence, one does indeed find that one synapse grows while the other shrinks, but not in an anti-symmetrical way. This is shown in Fig. , where we ran the analysis for very many time-steps. Both weights behave anti-cyclically and the observed oscillation grows as a consequence of the increasingly more influential auto-correlation term. Phase relationship between both synapses, however, remains the same (see inset in Fig. ). The result in Fig.  was obtained with an optimal choice of parameters ω0, ω1 and T leading to an almost ideal anti-symmetrical learning, shown in Fig. , which depicts the first learning steps. As mentioned, this situation is not generic. More often much more asymmetrical situations like in Fig.  are observed, which was obtained with the same setup only using a different value for T.

Symmetrical ICO-learning (Fig. ) produces a linear phase-relation, which is not shown here, but Fig.  shows instead that both weights develop in an accurate anti-symmetrical way. Weight development will exactly follow the weight change curve in Fig. . There is however a problem. Symmetrical ICO-learning does not anymore have one shared control parameter for the weight change, which for symmetrical ISO-learning was the derivative of the output. For symmetrical ICO-learning, two totally independent control parameters exist (the derivatives of the inputs). This can possibly lead to problems when wanting to control behavior with such a symmetrized ICO-rule.

7 Stabilizing ISO-learning with a third factor
ICO-learning is very stable but, as mentioned above, it is a form of non-Hebbian learning, where the output does not influence the learning. This may be undesirable in certain cases. Therefore, we undertook the effort to stabilize our (Hebbian) ISO-learning rule (). This can be achieved using a third factor, which we call the “relevance signal” R (Fig. ). For practical purposes most of the time we set it equal to x0, but one should realize that—like the reward line in TD-learning—R is indeed an independent signal. The signal R is meant to arise when for the animal/agent a behaviorally relevant event occurs.

The learning rule is similar to the ISO-rule (Eq. ): ISO3:  And therefore the weight change is ISO3:  where we introduced a new time interval TR, which regulates the timing of the third factor, and a very narrow filter signal with appropriate values of aR and bR (here we use a = 0.6, b = 0.66) to define the third factor. It is interesting that the auto-correlation term for ISO3 is in general unequal zero when using two filters pointing to a possible instability.

Figure  shows the signal structure. Let us assume that u1 reaches its maximum exactly at T. As υ′(t) = u′1(t), t &lt; T, we have . This is the situation depicted in panel (b). If we furthermore assume that the R signal is very short (e.g., using a delta-pulse for R) and that it also happens at T, then also the learning only takes place at this moment in time. Under these conditions, it is easy to see that we have totally eliminated the auto-correlation contribution. The outcome of panel c is obtained under these condition and ISO3 is stable (compare ISO and ISO3 in Fig. ). The inset shows again the relaxation behavior of ISO3 (step) for a single pulse-pair in comparison to ISO (curved), which demonstrates instantaneous relaxation of ISO3. Clearly this example is constructed as T is usually unknown such that  cannot be generally assured. Hence, it seems we have not gained anything so far by introducing ISO3. However, the situation changes when using a filter bank to spread signal x1 out in time (see also section on filter banks below). Then one can prove that the condition  will self-emerge as a consequence of the learning when using enough filters (). Thus, when using a filter bank ISO3 becomes a very stable method, indeed. For ISO3 we receive the learning curve in Fig. , which now only carries an LTP component. This behavior compares to TD-learning (see Fig. ).

8 Results when using a filter bank
The usefulness of all these rules as presented so far remains limited as most of the time the interval T between incoming inputs is not good enough known and might, in a behaving agent, even vary to quite some degree. Hence it is required to use a set of different eligibility traces E11,…,N to make sure that the earlier input is spread out over a long enough time such that the later input (x0 or r) can be correlated to it. Figure  depicts such a filter bank architecture for the ISO rule and panel b shows how the signals u11,…,N look like for a set of filters h.

Interestingly, convergence properties for the CL-rules are theoretically not affected when using a filter bank. It can be shown that for the ISO rule a set of filters h exist that fulfills a certain orthogonality criteria and ISO will then still converge for x0 = 0 (; ). The problem is that this is only an existence proof and nothing is currently known of how to actually construct this filter bank. Hence, when wanting to use ISO one has to fall back onto heuristic assumptions for the filter bank. Generally, however, this leads to the situation that the error-sensitivity of ISO can become larger, rendering this rule instable. The conclusion is that, while it has been the first differential Hebbian learning rule to be used in closed loop behavioral control (), ISO should only be used with great care anymore.

The properties of ICO and ISO3 are better. Both rules are stable and stability for x0 = 0 can be mathematically proved for both rules even when using filter banks (, ). These rules have now been successfully tested in a variety of different applications (; , ; ) and even chains of learning neurons can be constructed in a convergent way ().

Neuronal implementations of TD-learning generally solve the problem of unknown T by using a so called serial compound representation (, ; ; ). This is depicted in Fig. . Essentially it represents a tapped delay line by which the earlier stimulus is spread out across several input lines u11,…,N and for which one can even graphically show that TD will converge (Fig. ). This approach has been discussed in greater detail in . Such a rigid pulse protocol is not biologically realistic, though, and approaches exist which replace this by “smoother functions”, similar to the filter bank approach discussed above (). These signals do not affect the δ-error, and convergence can still be assured for 〈δ〉 = 0. As the filter response enter the learning via υ′, it remains unclear of how to construct more realistic filter banks for TD-learning which in a behaving system will still allow approaching 〈δ〉 = 0 in a reliable way. So far there is no theory existing for this.

9 Discussion
The main goal of this paper was to provide a road map through the different basic as well as extended RL and CL learning rules showing their fundamental mathematical properties in an open loop situation (hence without behavioral feedback). Table.  summarizes these observations. To this end, in the previous sections we have gone through a variety a temporal sequence learning rules first introducing their basic version and later some more advanced modifications. Three aspects were in the fore-front of the discussion: (a) convergence properties, (b) symmetry (hence, learning versus unlearning) and (c) the behavior of these rules when using a filter bank.
Overview over all learning rules discussed in this paper

Class		Two Inputs				
Type	Name	Output: υ	Rule: dw1/dt	Convergence	Comment	
CL	S&amp;B	w0x0 + w1x1	u1υ′	divergent	stimulus substitution	
	ISO	w0u0 + w1u1*	u1v′	x0 = 0, unstable!	symmetric, diff. Hebb	
	ICO	w0u0 + w1u1*	u′0u1	x0 = 0 	diff. Heterosyn.	
	ISO3	w0u0 + w1u1*		x0 = 0, (R = 0)	3-factor diff. Hebb	
RL	TD	w1x1		〈δ〉 = 0	critic only, no actions	
	TD-r	w0u0 + w1u1*		〈δ〉 = 0, x0 = 0	mixed Hebb + diff. Hebb	
	TD-a			〈δ〉 = 0	recursive rule	
Class		Filter Bank		Summary		
Type	Name	Output: υ	Convergence	General Comment	Use	
CL	S&amp;B	not applicable	not applicable	only of historical relevance	−	
	ISO		x0 = 0 for certain	unstable! As optimal hi are unknown,	−	
			unknown hi	convergence cannot be guaranteed, input control		
	ICO		x0 = 0	robust, heterosynaptic, input control	+	
	ISO3		x0 = 0, (R = 0)	robust, input control	+	
RL	TD	serial compound rep.	〈δ〉 = 0	robust, output control	+	
	TD-r	not tested so far	〈δ〉 = 0, x0 = 0	undesirable mix of Hebb &amp; diff. Hebb	−	
	TD-a	not tested so far	〈δ〉 = 0	not tested so far	?	
The asterisk * depicts identical equations within one column



Concerning (a) we have found that all TD-rules are stable if one can assure δ = 0 or leastwise . From the other rules the ICO and ISO3 rules are stable for x0 = 0. ISO is only theoretically stable but strongly affected by numerical artifacts.

Concerning (b) we can state that only the ICO-rule is truly symmetrical. Hence, with this rule it would be possible to implement learning at one input (the one that experiences +T) and unlearning at the other input (the one that experiences −T) at the same time. With ISO this is to some degree possible but growth and shrinkage are not the same, while all TD-rules are by construction not symmetrical.

Concerning (c) we observe that ICO and ISO3 will maintain their convergence properties for x0 = 0. TD-learning will converge when using a serial compound representation (see Fig. ). This sitation is very similar to a state-space tiling performed in machine learning version of TD. On the other hand, no clear theoretical results exist for TD, when using different, more neuron-compatible filters for the eligibility traces.

An additional consideration concerns the difference between input- and output-control. The CL-rules enforce convergence via x0 = 0, which represents an input condition. TD learning converges via output control (). This difference may lead also to differences for control applications as discussed elsewhere ().

9.1 Biological relevance
To what degree are the above discussed models related to temporal sequence learning mechanisms in the brain. At first we notice that certainly all of them are at a much higher level of abstraction as compared to the biophysics of synapses. But, for example, we note that the learning curve of ISO or ICO learning resembles curves measured for spike timing-dependent plasticity (; ; ). Hence it is possible to model STDP with such a formalism (; ).

Furthermore, it has long been discussed that TD-learning could be related to dopaminergic responses in the brain. Especially the behavior of some cells in the substantia nigra and ventral tegmental area (VTA) suggest that they represent the δ-error of TD-learning. Models, which behave in a similar way have been made by Suri and co-workers (; , , ; ) and we direct the reader to this literature for an in-depth discussion. The problem, with these models, however, is that it is difficult to find appropriate biophysical equivalents for the implementation of the TD-rule.

Concerning the CL-rules, there are different degrees of realism. For the ICO-rule we find that it represents a special case. This is due to the fact that ICO-learning implements plain heterosynaptic plasticity and this is found only at a few specialized synapses (; ). Heterosynaptic plasticity is usually associated with modulatory processes and not directly with Hebbian learning. This is different for the ISO-rule, which uses the derivative of the output to control learning and therefore represents conventional (differential) Hebbian learning.

The instability of the ISO-rule, however was the reason for us to design ISO3, which is a form of (differential) Hebbian learning using a three-factor learning rule (). Such three-factor rules have recently also been discussed in conjunction with the Dopaminergic system of the brain (). Also, since it is a Hebb-rule, it is better suited to be matched to our knowledge concerning LTP and LTD. Furthermore, we found, quite unexpectedly, that for weight stabilization ISO3 can use one interesting aspect of the behavior of dopamine cells in the substantia nigra and VTA (): These cells appear to learn anticipating a reward, whereby the temporal occurrence of their response shifts from (first) tx0 to (later) tx1. When doing this with our relevance signal, we found that learning stops and that weights become essentially stable even without setting x0 = 0 (data not shown). Bringing the average TD-error
 down to zero does require the dopamine responses to take a very specific shape whereas for stabilizing weights in ISO3 it is enough to get a somewhat sharp response at x1 while loosing the R-signal at x0. This seems to be better in conjunction with the properties of dopaminergic responses which do not appear to fulfill high accuracy requirements.

Thus, an experimental question now arises: Do the dopamine cells in the substantia nigra and/or VTA represent the δ-error in TD-learning or do they reflect a relevance signal to be used as a third factor in the learning?



Acknowledgment
The authors acknowledge the support of the European Commission, IP-Project “PACO-PLUS” (IST-FP6-IP-027657). We are grateful to T. Kulvicius for help with some figures.

Open Access This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.

A quasi-static approach is commonly assumed for such calculations (Dayan and Abbott 2003) and is justified as long as μ is small. It is needed for neglecting the derivative of the weight w1 on the right hand side of the equation and for neglecting the variability of the homogeneous solution for the calculation of the inhomogeneous part.

