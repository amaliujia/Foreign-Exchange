1 INTRODUCTION AND RELATED WORK
Finding relevant information is one of the most important challenges in our time. In particular in life science and chemical research a huge amount of new publications, research reports and patents is produced every year. For users of huge text corpora like MEDLINE, document categorization, ranking and finding entity-related information is an important help in their daily research and work life. The automated identification of entities of interest in text in a domain and their mapping to database entries strongly improves information retrieval, information extraction and information aggregation. For such tasks tools have been successfully developed in the last decades especially for finding mentions of proteins and genes. Those provide basic methods to extract e.g. protein–protein relations. For that field of interest, the BioCreative competition (Hirschman et al., ) provides an evaluation of state-of-the-art techniques on publicly available data sources. Dictionary-based systems allow a direct mapping of the recognized entities to reference objects (e.g. EntrezGene identifiers for genes). An inherent drawback of such approaches, however, is the dependence on the quality and completeness of the dictionary and the methods of the underlying algorithm to recognize spelling variants and to resolve ambiguous names.

Dictionary independent methods (rule-based systems as well as machine learning-based system) are well suited to find names where no comprehensive dictionary is available. An example for a rule-based approach is to find mentions matching a given set of regular expressions. Machine learning approaches are based on an annotated training set from which statistical information can be obtained about the inherent dependencies in the data. This extracted information can then be applied on unseen data to classify word tokens, i.e. to label parts of text with different classes. The best approaches in the BioCreative sub task of gene mention recognition have an F1 measure between 86% and 87% (Wilbur et al., ). From 21 submissions, 11 use conditional random fields (CRF), a machine learning method based on undirected graphical models (Bishop, ) which leads to competitive results, in our hands 86.33% F1 measure (Klinger et al., ).

Finding mentions of chemical compounds in text is of interest for several reasons. An annotation of the entities enables a search engine to return documents containing elements of this entity class (semantic search), e.g. together with a disease. This can be helpful to find relations e.g. to adverse reactions or diseases. Mapping the found entities to corresponding structures leads to the possibility to search for relations between different chemicals. This enables a chemist to search for similar structures or substructures and combine the knowledge in the text with knowledge from databases or to integrate other tools handling chemical information (e.g. solubility or mass calculation).

Chemical names can be distinguished into different classes: to deal with complex structures, different methods of nomenclature are used, e.g. mentions of the sum formula or names according to the Simplified Molecular Input Line Entry Specification (SMILES; Weininger, ) or the successor of SMILES, the IUPAC International Chemical Identifier (InChI). Because of a limited readability of such specifications for humans, trivial names and the nomenclature published by the International Union of Pure and Applied Chemistry (IUPAC; McNaught and Wilkinson, ) is commonly applied (Eller, ) in text. Also combinations of the different types of names as well as abbreviations, especially of often used substances, are in use.

Trivial names can be searched for with a dictionary-based approach and directly mapped to the corresponding structure at the same time. For example, the dictionary-based named entity recognition system ProMiner (Hanisch et al., ) uses a DrugBank (Wishart et al., ) dictionary for the recognition of drug names in MEDLINE abstracts (Kolářik et al., ). Other systems use the drug dictionary from MedlinePlus (e.g. EbiMed; (Rebholz-Schuhmann et al., ) for the recognition of drug names.

For other representations of chemical structures like SMILES, InChI or IUPAC names such an enumeration is only possible for the most common substances. The full chemical space cannot be enumerated. Several systems address that problem regarding chemical entities with a variety of approaches.

Narayanaswamy et al. () describe a manually developed set of rules relying upon lexical information, linguistic constraints of the English language and contextual information for the detection of several entity classes. The reason for choosing this approach is stated as the lack of an annotated corpus. The evaluation was done on a small hand-selected corpus containing 55 MEDLINE abstracts selected by searching for acetylates, acetylated and acetylation. They found 158 chemical names from which 22 were ambiguous and classified into different classes and 13 chemical parts with two ambiguous ones. The F1 measure for the first is 90.86% (93.15% precision, 86.08% recall). The latter has an F1 measure of 91.67% (100% precision, 84.62% recall).

Similarly, (Kemp and Lynch, ) identify chemical names in patent texts with handcrafted rules using dictionaries with chemical name fragments. They claim to identify 97.4% from 14 855 specific chemical names in 70 patent descriptions taken from documents from the IPC class CO 7D. The false positive rate is reported to be 4.2%.

The concept described by Anstein et al. () for which the preconditions are described by Reyle () uses a grammar for the analysis of fully specified (e.g. 7-hydroxyheptan- 2-one), trivial (e.g. benzene) and semi-systematic (e.g. benzene-1,3,5-triacetic acid) as well as underspecified (e.g. deoxysugar) compound names. The advantage of that approach is that the grammatical analysis can be used as a basis for a conversion to the chemical structure. A possible problem is the difficulty to recognize names not following the specification to a certain degree as well as the completeness and maintenance of a changing standard.

A molecular similarity search is used by (Rhodes et al., ) to enable a user to ‘search for related Intellectual Property’ in US patents based on a specified drawn molecule. They report to find 3 623 248 unique chemical structures from 4 375 036 US patents. The absolute numbers of found patents for the top 25 drugs listed by Humana () are given.

The program developed by (Sun et al., ) focuses on finding sum formulas like CH3(CH2)2OH in text using support vector machines (Schölkopf and Smola, ) and CRFs (Lafferty et al., ).

In the work of Corbett et al. (), first-order Hidden Markov Models (Rabiner, ) implemented in the toolkit LingPipe are combined with other methods for the identification of chemical entities. The program finds e.g. structural classes, atoms and elements, fragments, trivial names, SMILES and InChI as well as IUPAC names. They give an inter-annotator F1 measure of 93% for chemical names on their annotated corpus. The performance is evaluated on different corpora, recall rates are between 69.1% and 80.8% and precision rates beween 64.1% and 75.3% (Corbett and Murray-Rust, ). A seperate evaluation of the included named entity recognition modules from the toolkit LingPipe results in an F1 measure of 74% (Corbett et al., ). To our knowledge, their implementation—the open source program OSCAR3 (Open Source Chemistry Analysis Routines; Corbett, )—is the only software available to the academic community.

We prefer to have a method identifying IUPAC and IUPAC-like names only and to have additional approaches to recognize other chemical name classes (e.g. brand names or elements): IUPAC and IUPAC-like names can be recognized based on their morphological structure with higher performance than with methods based on dictionaries (Kolářik et al., ). We therefore introduce a system for the recognition of IUPAC and IUPAC-like names while trivial names are found with a dictionary approach not described here. These IUPAC-like terms do not only include correct IUPAC names but also names not following the nomenclature strictly. This enables a higher recall regarding mentioned chemicals, which is important for document retrieval purposes.

In addition to the correct recognition of IUPAC and IUPAC-like names, the aim is to transform these names using name-to-structure converters to allow the usage of chemical tools on the extracted data. Therefore enumerations have to be detected with all parts while modifying tokens (e.g. substitutes, analogs) have to be tagged separately (cf. ). We use a CRF approach and present our development of a training corpus as well as our experiences regarding inter-annotator agreement in comparison to the work of Corbett et al. (). Next to the training corpus we describe test corpora especially on MEDLINE and on patents.
Example abstract with tagged entities (PMID 9240357; Guzikowski et al., ). IUPAC entities are depicted in red while MODIFIER entities are shown in blue.



Additionally, to the presentation of the results an exhaustive analysis of the influences of different CRF orders and offset conjunctions is shown and the impact of the different feature sets on the results are evaluated and discussed.

2 METHODS
2.1 Overview
We apply a CRF to build a model for finding IUPAC and IUPAC-related MODIFIER entities. A training corpus has been annotated by two independent annotators and the inter-annotator agreement is discussed in . The model selection is performed by bootstrapping (Efron and Tibshirani, ) and evaluated on two independent test corpora, one consisting of sampled abstracts from MEDLINE, the other one on hand selected paragraphs from bio-chemical patents. We analyze the use of name-to-structure converters as a basis of a possible normalization, a mapping of the found entities to a unique structure.

2.2 Entity types
The entities in which we focus are IUPAC and MODIFIER mentions. As described, chemical entities in general are named following different nomenclatures which are also combined by the authors of biomedical texts. Only concentrating on correct IUPAC terms is not sufficient, so we define a IUPAC entity to be a chemical substance mentioned in a IUPAC-like manner. Additionally to correct IUPAC names, it includes IUPAC names in which a part is abbreviated, fragments and group names. In  an example abstract from MEDLINE with annotations of the two entity classes is shown. Next to full names like ‘1,2,3,4-tetrahydronaphthalene-1-carboxylic acid’ or ‘4-[[(3-chlorophenyl)amino]methyl]-6,7-dihydroxychromen-2-one’, fragments, e.g. in enumerations, are tagged separately like ‘acridine-4-’ and ‘phenazine-1-carboxamide’ in ‘…both the acridine-4- and ‘phenazine-1-carboxamide series…’ or ‘3α-[bis(4-fluoro-’ and ‘4-chlorophenyl)methoxy] tropane’ in ‘…N- and 2-substituted-3α-[bis(4-fluoro- or 4-chlorophenyl)methoxy]tropane…’.

The alternative to the separate way of annotating parts in enumerations would have been an annotation including the connecting word (in that example ‘or’). This is not meaningful because parts of names are sometimes divided by long text passages. With our kind of annotation, a possible enumeration resolution of the found parts in the text is prepared.

The MODIFIER entity describes similarities to a mentioned substance like in ‘[IUPAC-entity] analogues’ or ‘[IUPAC-entity] modifier’ or ‘3-substituted-IUPAC-entity’.

2.3 Corpus generation and inter-annotator agreement
Three main corpora are generated for building the model and evaluating our approach following a developed annotation guideline. A training corpus consisting of MEDLINE abstracts (abbreviated as TrainM), a test corpus containing MEDLINE abstracts (TestM) and a test corpus made up of parts of patents (TestP).

The training corpus is built in two steps. First, a preliminary corpus (abbreviated as Trainpr) is built in the same manner as described by Friedrich et al. (). For that, in the BioCreative training corpus (Hirschman et al., ), the gene and protein names are replaced by randomly selected correct IUPAC names from PubChem (NCBI, ). This leads to an artificial corpus with 15 000 sentences with 1 216 341 tokens. It includes 24 325 entities. On that corpus a CRF is trained and used for tagging 10 000 sampled abstracts from MEDLINE. From these, 463 abstracts are selected which include 161 591 tokens in 3700 sentences with 3712 IUPAC and 1039 MODIFIER entities.

For evaluation of the system, 1000 MEDLINE records with 124 122 tokens in 5305 sentences are sampled equally distributed from full MEDLINE containing 151 IUPAC and 14 MODIFIER entities resulting in the corpus TestM.

Passages from 26 patents dealing with chemical processes were hand selected according to occurring enumerations of chemicals, especially using different and mixed nomenclatures to detect possible problems. These paragraphs consist of 4309 words in 152 sentences with 411 IUPAC entities forming the corpus TestP.

The training corpus is annotated by two independent annotators. An assessed inter-annotator F1 measure for the IUPAC entity is relatively low with 78% [in contrast to 93% claimed by Corbett et al. ()]. One reason for the difference in comparison to Corbett and his colleagues is our differentiation of the IUPAC entity to other chemical mentions, which is not always easy to decide while all chemical mentions in the corpus generated by Corbett are combined in one entity. Another reason are the different experience levels of our annotators: while the first annotator collaborated on the development of the annotation guideline and annotated several corpora, the second annotator based his annotations directly on the provided guideline.

For building the conclusive training corpus both annotations are combined by an independent person. The F1 measure between the resulting training corpus (TrainM) and the first-annotated corpus is 94%.

2.4 Conditional random fields
CRFs (Lafferty et al., ; McDonald and Pereira, ) are a family of probabilistic, undirected graphical models for computing the probability  of a possible label sequence  given the input sequence . In the context of named entity recognition this observation sequence  corresponds to the tokenized text. This is the sequence of tokens which is defined by a process called tokenization—splitting the text at white space, punctuation marks and parentheses. A straightforward idea for the tokenization of IUPAC names is to keep the whole name together to use the sheer length for their identification. That is not possible because of leading and successive brackets and other symbols as well as often used wrong white spaces (based on e.g. converted line breaks). So we use a very fine tokenization, also splitting at all number-letter changes in the text.

The label sequence is encoded in a label alphabet similar to ℒ={I-&lt;entity&gt;,O,B-&lt;entity&gt;} where yi=O means that xi is not an entity, yi=B-&lt;entity&gt; means that xi is the beginning of an entity and yi=I-&lt;entity&gt; means that xi is the continuation of an entity. In our case we use the alphabet

as described in . An example for an observation sequence with a label sequence is depicted in .
Example for observation and label sequence for the text snippet: ‘…of cyclohepta-1,3-diene …’ after tokenization.



A CRF in general is an undirected probabilistic graphical model

where Ψj are the different factors given through an independency graph like in  (Kschischang et al., ). These factor functions combine different features fi of the considered part of the text and the label sequence. We mainly use morphological features of the text tokens for every possible label transition. A subset of the used features is depicted in . They usually have a form similar to

The feature set used in our approach is described in .
First-order linear-chain CRF.


Features used in the CRF

Name	Explanation	
Static morphol. features	Reg.Ex.	
All Caps	[A-Z]+	
Real Number	[-0-9]+[.,]+[0-9.,]+	
Is Dash	[- – — −]	
Is Quote	[,, “ ” ” ‘ ’]	
Is Slash	[\ /]	
Autom. generated features		
Autom. Prefixes/Suffixes	Automatic generation of a feature for every token: match that prefix or suffix (length 2)	
Bag-Of-Words	Automatic generation of a feature for every token: match that token	
Spaces		
Spaces_left	white space preceding token	
Spaces_right	white space following token	
Lists		
Prefix/Suffix lists	Prefixes and suffixes (length 4) of intermediate or last words generated from IUPAC names	


A special case of the general CRF, in fact the one shown in , is the linear-chain CRF where the factors are given in the form

so that the CRF can be written as

The normalization to [0,1] is given by

Here 𝒴 is the set of all possible label sequences.

To compute the normalization factor, the forward-backward algorithm known from Hidden Markov Models (Rabiner, ) can be incorporated. Optimization of the parameters (training) can be done by applying the limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm (L-BFGS; Nocedal, ) on the convex function ℒ(𝒯) with the training data 𝒯:

These algorithms can also be used in CRF with a higher order, given by

where q is the order of the CRF (cf. ).
Second-order linear-chain CRF.



Our own implementation of the named entity recognizer for IUPAC terms is based on MALLET (McCallum, ), a widely used and successfully applied system for linear-chain CRF. A more detailed description of these models and their relation to other graphical models is e.g. given by Klinger and Tomanek ().

2.4.1 Feature set
Many of the evaluated features are extracted by standard methods, especially the morphological ones. Some of them are shown in . Next to these commonly used features we incorporate special IUPAC-related features. These are the membership of a token to a list of often used prefixes and suffixes of length four in IUPAC names or a list of typical last tokens of the names. These lists are extracted from all IUPAC names mentioned in the data available from PubChem (NCBI, ).

The list of prefixes of length 4 has 714 members, the list of suffixes of the same length has 661 members. Another list includes 300 suffixes of the last tokens of IUPAC names to improve the detection of the end of a IUPAC name. The general idea of these lists is to provide the system with a possibility to generalize in excess of the training data. Another feature usually not used in the context of other entities is the specification of a token being preceded or succeeded by white space. This is important especially in enumerations or abbreviations of IUPAC names or trivial names to separate them from each other, in particular with reference numbers like shown in . This feature is necessary due to the need of a fine tokenization.

Additionally, we use the so-called offset conjunction (OC) that adds features of the preceding and succeeding tokens for every token, incorporating contextual information to the token to be labeled.

2.5 Conversion of IUPAC names to structures
To normalize the found names, one solution is to convert them to a structure representation. Several tools have been developed for that task.

Eigner-Pitto et al. () show a short evaluation of three commercial tools. One is LexiChem (OpenEye, ) by OpenEye, a product capable of conversions from IUPAC names as well as other names to structures and vice versa. Another program is ACDName by ACDLabs, which generates chemical structures from systematic names, derivatives, semi-systematic and trivial names as well as incorrect names, not strictly following the nomenclature (ACDLabs, ), but it focuses more on correct names than the program Name=Struct by CambridgeSoft (CambridgeSoft, ; Eigner-Pitto et al., ).

We use the open source converter included in OSCAR3, called OPSIN, the only software to our knowledge, which is freely available for academic evaluation purposes. It converts names to the Chemical Markup Language (CML, Murray-Rust ()) which we translate to SMILES using the Chemistry Development Kit (CDK; Steinbeck et al., ).

3 RESULTS
In a first step a CRF is trained on the preliminary, tweaked corpus Trainpr mentioned in  and evaluated by 50-fold bootstrapping. The result is an F1 measure of 97.92% (98.08% precision, 97.76% recall) with a first-order CRF with first-order offset conjunction and the same parameter set as described in . These results are comparable to those published by Friedrich et al. (). Evaluating this model on the annotated MEDLINE training corpus TrainM shows a low F1 measure (with 19.5%) and 38.4% recall. The performance on the sampled MEDLINE test corpus TestM is even worse with 1.1% F1 measure and a recall of 29.1%. These results show that there is a fundamental difference in tagging the tweaked corpus Trainpr (which seems to be simple, considering the F1 measure) and real world texts (as TrainM and TestM). The analysis of the different corpora shows two main problems: On the one hand, only correct IUPAC names are included in Trainpr, but fragments occur frequently in real text. On the other hand, a big problem are missing negative examples in the tweaked training data representing what is not a IUPAC name: nearly all isolated numbers, single letters, expressions in or around brackets are found wrong.

Based on the experiences on the tweaked training corpus Trainpr, a CRF is trained on the annotated training corpus based on MEDLINE abstracts TrainM using a selected parameter set. The evaluation of the different parameters is given.

3.1 Parameter selection
For model selection, the impact of the following parameters of the CRF are evaluated by applying 30-fold bootstrapping on the training set TrainM:
features representing the text like Bag-Of-Words or morphological features (cf. ),

the order of the CRF and

the order of the offset conjunction.


The feature set of the system with the best performance consists of automatically added features based on Bag-Of-Words as well as Autom. Prefixes/Suffixes of length two. Additionally, the membership to Prefix/Suffix lists containing prefixes or suffixes of length four of last or intermediate tokens from IUPAC names is considered. From the set of static morphological features, All Caps, Real Number, Is Dash, Is Slash and Is Quote are used. The Spaces features to determine if the token is preceded or succeeded by white space is also included. Many other features, mainly from the field of gene and protein recognition were also tested, e.g. mapping the token to regular expressions representing greek letters, combinations of alpha-numerical symbols, natural numbers, etc. For lists of different features see McDonald et al. (), McDonald and Pereira (), Settles () and Klinger et al. (,).

To evaluate the impact of the different features we omit one from the best feature set in several experiments () and train models only with small feature sets (depicted in ). The automatically generated features Bag-Of-Words and Autom. Prefixes/Suffixes have the highest impact on the performance together with the Spaces feature. Especially the last one is essential to obtain good results with impacts between 6.5% (CRF 2, OC 2) and 13.64% (CRF 1, OC 0). In contrast, the static morphological features and the Prefix/Suffix lists bring nearly no loss omitting them and low results when used as the only feature. Nevertheless, together with the feature Spaces, the results are surprisingly high (70% F1 measure). Interesting is that using only Autom. Prefixes/suffixes or Bag-Of-Words together with the Spaces feature and CRF order 3 and offset conjunction order 2 results in an F1 measure of 76.03% or 79.31%, respectively.
Results on the training data TrainM with 30-fold bootstrapping with different feature sets, different orders q of the CRF (given as CRF q above) and different orders p of the offset conjunction (given as OC p). The best results were obtained with the feature set presented in . For more details see  and .


Results on the training data TrainM with some very small feature sets on different orders of the CRF and offset conjunctions. In contrast to , where variations of a larger feature set are shown, the importance of features is presented in the context of very small feature sets. (Note the different scales between  and .)




We evaluate different configurations of the features with different orders of offset conjunction (adding context in the form of features of the last p and next p tokens, where p is the order of the offset conjunction) as well as the order of the CRF, which includes information from the last q labels (q is the order of the CRF). The results of some of the features for different orders of offset conjunction and CRF can also be seen in . The importance of the different features is nearly the same for all the different orders. The divergence in the results is high for different feature sets, but it is also very important to have the context information provided by the offset conjunction. The best F1 measure can be obtained with an offset conjunction of order 2 and a CRF order of 2 or 3. The difference between a CRF without an offset conjunction (i.e. order 0) to a CRF with order 1 offset conjunction are much higher than between order 1 and order 2 offset conjunctions. The increase of the order comes along with a high increase in the number of weights λi [compare to Equation () and ()]. We have (for the CRF with order 3) 119 884 weights without an offset conjunction, 315 377 with an offset conjunction order of 1 and 521 179 weights with an offset conjunction order of 2. This corresponds to the training and tagging durations depicted in .
Results on the sampled MEDLINE corpus TestM with different orders q of the CRF (given as order q) and orders p of the offset conjunction (given as OCp). The upper chart shows the F1 measures for the different configurations, the lower one the tagging and training durations.



Inspecting the tagging errors, we find that especially boundary errors at the end or at the beginning of the name are more frequent for a lower order of the offset conjunction. Other taggings that can be correctly identified with an offset conjunction order of 2 are formulations like ‘… through the 7- or 12-methylene carbon with …’ where the high context information is necessary to classify ‘7-’ correctly. A similar example is ‘… 2,3-substituted …’ with a tagging of ‘2,3’ as IUPAC with an offset conjunction of 1 but a correct result with an offset conjunction of 2.

3.2 Evaluation of the named entity recognition
Using the best configuration identified in the previous section, the resulting model is evaluated on the sampled MEDLINE test corpus TestM. In  different orders of the CRF and the offset conjunction together with tagging and training durations are depicted. Similar to the results estimated with bootstrapping on the training corpus TrainM, highest performance is obtained with the most context information included by a CRF order of 3 and an offset conjunction order of 2. The F1 measure for IUPAC entities is 85.6% with a precision of 86.5% and a recall of 84.8%. The MODIFIER entities are found with an F1 measure of 84.6% (91.7% precision and 78.6% recall). Higher orders have not been applied because of prohibitive training durations. However, it can be seen that our best result is obtained at the expense of a high training time and, what is more important, on a higher tagging time of 307 s then other configurations of the CRF. For tagging a higher amount of data like the full MEDLINE database one could prefer to use a faster configuration like the one with order 2 and offset conjunction of 1 which only takes 215 s for tagging the test corpus. The F1 measure for IUPAC entities is lower with 77.7%, but the recall is nearly on the same level with 82.1% (MODIFIER: 55% precision, 78.6% recall). It can be concluded, that there is a trade-off between tagging time and performance, so it depends on the application which configuration should be preferred. The analysis of the errors show frequent problems in the recognition of short chemical names. On the one hand, chemical names are recognized by the system which are not specified as IUPAC-like. On the other hand, short names, similar to trivial names, specified as IUPAC-like by the annotators are most frequently unrecognized by the system. Nearly 50% of the other false positive errors are boundary errors. In addition, names morphological similar to IUPAC names like enzymes (e.g. ‘2-phospho-D-glyceratehydro-lyase’ or ‘pyruvate O2-phosphotransferase’) are detected as false positive matches.

Applying the best system trained on the MEDLINE training corpus, TrainM, for tagging the patent test corpus, TestP, shows a decrease in F1 measure in comparison to the MEDLINE test corpus TestM due to the bias of hand selecting difficult paragraphs instead of sampling from a set of sentences or text snippets. We get an F1 measure of 81.5% with a precision of 77.2% and a recall of 86.4%.

3.3 Annotation of full MEDLINE
We performed a run of the best CRF model on the full MEDLINE with 16 848 632 MEDLINE article entries (version as of July 13, 2007). In these entries, we have 8 975 073 abstracts. We tag titles and abstracts, altogether 2.2×109 tokens, in which there are 1 715 263 IUPAC entities in 875 102 MEDLINE database entries. The tagging is performed on a computer cluster using 48 machines with two Opteron AMD double core processors with 2.6 GHz and 8 GB main memory on each machine in 76.65 h (3.19 days). The operating system is Suse Linux Enterprise Server 9 (x86 64) with the Sun N1 Grid Engine 6.

From the found IUPAC entities, only 142 181 could be transformed to a structure (16.24%). The top 15 found terms from MEDLINE are shown in , the top 5 of the converted structures in  together with the most often used terms which lead to the normalization. To get an upper bound of convertible IUPAC names, we sample 100 000 correct names from data provided by NCBI (). From these, 30 028 (30%) are converted to structure information by OPSIN.
Top 15 found terms with their number of occurrences

Frequency	Name	
16811	N-methyl-D-aspartate	
15275	5-hydroxytryptamine	
11690	5-fluorouracil	
9001	6-hydroxydopamine	
7023	glucose-6-phosphate	
6685	N-ethylmaleimide	
5932	N-acetylcysteine	
5178	12-O-tetradecanoylphorbol-13-acetate	
5032	methyl	
4742	N-acetylglucosamine	
4311	benzo[a]pyrene	
4164	3-methylcholanthrene	
3991	4-aminopyridine	
3931	2,3,7,8-tetrachlorodibenzo-p-dioxin	
3979	5-hydroxyindoleacetic acid	



4 SUMMARY AND DISCUSSION
In this article, we present our approach of finding IUPAC-like terms in text with CRF. We demonstrate that our IUPAC recognizer identifies entities with an F1 measure of 86.5% on a sampled independent test corpus built from MEDLINE. This corpus gives an estimation for all available abstracts from that database. These results show that restricting the recognition to the special class of IUPAC-like terms instead of all mentions of chemical names as focused on in Oscar3 (Corbett et al., ) increases the performance.

Using a tweaked corpus with correct IUPAC names shows that incorporating only complete IUPAC names in the training corpus is not sufficient. Obviously, the challenge is to recognize fragments and parts of IUPAC names. An error analysis of the final system on MEDLINE shows that boundary problems and the recognition of shorter chemical names lead to the main performance loss. This may be founded in ambiguities in the training data regarding these names and should be considered in a further extension of the training corpus. Preliminary results on an extended annotation of short names show an increase of precision to 91.4% (Kolářik et al., ).

When the IUPAC recognizer is applied to a hand-sampled patent corpus containing long enumerations and mixtures of different chemical nomenclatures the drop in performance is unexpectedly low with an F1 measure of 81.5%. Apparently, the loss of F1 measure in comparison to the MEDLINE corpus is due to a loss of precision rather then recall. Typical problems are finding the right borders of the chemical names in enumerations. From these results we cannot generalize that it is harder to find IUPAC names in patents than in abstracts.

In the feature evaluation we show that automatically generated features like Bag-of-Words and Autom. Prefixes/Suffixes together with Space information are the most important features influencing the performance of the system. The usage of combinations of these features alone e.g. Space together with prefixes and suffixes result in an F1 measure of 76.03%. In contrast, the static morphological features which are usually very important for a good generalization (together with other morphological features), in particular on the entity class of genes and proteins (Klinger et al., ) do not have such a high impact here. Remarkably, the Prefix/Suffix lists (used for generalization purposes) appear to be of very low importance indicated by nearly no loss when left out. When used as the only feature, no positive result can be obtained. However, when combined with the feature Spaces, the results are surprisingly high (70.71% F1 measure).

Higher orders of the CRF in combination with high order offset conjunctions lead to the best results observed (F1 measure 85.6%) on the MEDLINE test corpus with an CRF order 3 and an offset conjunction of 2. On this corpus also the direct dependency of training and labeling durations to the orders of CRF and offset conjunction are shown (cf. ).

Despite of the analysis given here for IUPAC entities, the open question remains, in which cases a representation of context information on the labels should be preferred in comparison to a representation of context information in the text, in form of features, used here by incorporating offset conjunction. To our knowledge, no deeper analysis is published about that topic so far. Our own experiments with different orders of CRF and offset conjunction in the field of gene and protein names showed that with higher orders the results tend to get worse, probably because of more needed training data when more complex dependencies are modeled (data not shown here).

In a final test, the full MEDLINE was labeled showing the scalability of the implementation. The highest frequency (without normalization) is almost 17 000 mentions of one term (). A conversion of the names to its corresponding structure show that only a minor part (below 20%) can be processed (without evaluating the correctness of the conversion). From the 15 most frequent chemical names only one can be converted (4-aminopyridine; cf. ). Even from correct names provided by the NCBI in the database PubChem, only 30% can be converted. Unfortunately, it is not allowed to evaluate the conversion rate of the commercial tools for academic applications. Difficulties in the name to structure conversion are mixed nomenclatures and formally incorrect IUPAC and chemical names instead of correct nomenclature. We conclude that name-to-structure conversion in its current form seems to be a persistent scientific challenge.
Top 5 found converted structures [applying OPSIN and CDK, drawn with Marvin (ChemAxon, )] with their frequency and the frequency of occurrences of the top 3 terms which lead to the SMILES string

	


In the future it is necessary to combine different existing tools and programs to be developed which find mentions of trivial names, formulas, IUPAC names, InChI, SMILES, group names, etc. and determine the intersection in their results and enable them also to find combined terms (e.g. in which part of the names follows the nomenclature of SMILES and another part follows IUPAC nomenclature). For that purpose, a representative, comprehensive test corpus including all these entities has to be developed.

Another goal is to combine the knowledge in drawn structures with the information in text, using the results provided by tools like chemOCR (Algorri et al., ; Zimmermann et al., ) which converts drawn structures into computer interpretable representations.

1http://redpoll.pharmacy.ualberta.ca/drugbank/

2U.S. National Library of Medicine (), http://mplus.nlm.nih.gov/medlineplus/druginformation.html

3http://alias-i.com/lingpipe/

4http://oscar3-chem.sourceforge.net

5The colors here show the entity: red for IUPAC entities, blue for MODIFIER entities

6Number of sentences is detected with the JULIELab sentence splitter (http://www.julielab.de/,Tomanek et al., ).

7In the I,O,B-format like mentioned above for the existence of one entity there are eight possible transitions: B→B, O→O, I→I, B→I, I→B, I→O, B→O and O→B.

8http://www.eyesopen.com/products/toolkits/lexichem.html

9Version of October 11, 2006, http://oscar3-chem.sourceforge.net

10Version 1.0.1 of June 26, 2007, cdk.sourceforge.net/

ACKNOWLEDGEMENTS
We thank all our co-workers, especially Erfan Younesi, Harsha Gurulingappa for annotation and Katrin Tomanek (University of Jena) and Theo Mevissen for fruitful discussions.

Funding: This work has been partially funded by the MPG-FhG Machine Learning Collaboration (http://lip.fml.tuebingen.mpg.de/).

Conflict of Interest: none declared.

