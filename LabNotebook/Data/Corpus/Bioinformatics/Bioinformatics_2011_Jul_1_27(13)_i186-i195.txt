1 INTRODUCTION
To further our understanding of the cellular processes shaping the response of microbial cells to changes in their environment requires the study of the interactions between gene expression and metabolism. In recent years high-throughput datasets comprising simultaneous measurements of metabolism (fluxes, metabolite concentrations) and gene expression (protein and mRNA concentrations) have become available (; ). These datasets provide a rich store of information for modeling the dynamics of the biochemical reaction systems underlying cellular processes. In particular, they promise to relieve what is currently a bottleneck for modeling in systems biology, obtaining reliable estimates of parameter values in kinetic models (; ).

Notwithstanding these experimental advances, parameter estimation remains a particularly challenging problem, among other things due to incomplete knowledge of the molecular mechanisms, noisy and partial observations, heterogeneous experimental methods and conditions, and the large size of networks (). As a consequence, the models may not be identifiable, may not generalize to new situations due to overfitting, and nonlinear rate functions may make them cumbersome to analyze. This has led to the proposal of simplified kinetic modeling frameworks, including linlog kinetics (), loglin kinetics (), power-law kinetics (), and more recently, convenience kinetics ().

Linlog models are a particularly interesting choice for modeling metabolism (; ). Simulation studies on the level of both individual enzymatic reactions () and metabolic networks (; ; ) have shown that they provide reasonable approximations of classical enzymatic rate laws. Moreover, with the help of a recent genome-scale linlog model of yeast metabolism, parametrized using previously-published kinetic models, it has been possible to identify key steps in the network, that is, reactions exerting most control over glucose transport and biomass production ().

A major advantage of linlog models is that, when measurements of fluxes, enzyme concentrations and metabolite concentrations are available, the parameter estimation problem reduces to multiple linear regression (). Power-law models, up to a logarithmic transformation, and loglin models also have this convenient property. However, the performance of regression approaches quickly degrades in the presence of missing data, as is often the case in high-throughput datasets due to experimental limitations or instrument failures.

In order to deal with this problem, we propose in this article a maximum-likelihood method for the identification of linlog models of metabolism from incomplete datasets. The specific contributions of the paper are 2-fold. On the theoretical side, we develop a method for the optimization of the likelihood based on Expectation Maximization (EM) (). The method is constructed for linlog models, but is more generally applicable to other approximate kinetic models whose identification can be formulated as a regression problem. In particular, we derive analytical expressions for the expectation step that are well-suited for numerical maximization. This guarantees the applicability of the approach even when modeling large networks. We show by means of simulation experiments on synthetic data that our approach outperforms both regression and a reference method from statistical literature for dealing with incomplete data, multiple imputation (; ). In comparison with earlier work on treating incomplete high-throughput datasets (; ), our aim is not to estimate the missing values, but rather to improve the estimation of the model parameters from the incomplete datasets. This is a different problem that necessitates the development of novel methods.

On the biological side, we apply the method to a linlog model of central metabolism in Escherichia coli, consisting of some 23 variables. We estimate the 100 parameters of this model from a high-throughput dataset published in the literature (). The data consists of measurements of metabolic fluxes and metabolite and enzyme levels in glucose-limited chemostat under 29 different conditions such as wild-type strain and single-gene mutant strains or different dilution rates. Standard linear regression is difficult to apply in this case due to missing data, which disqualifies for 7 reactions too many datapoints, leaving a dataset of size inferior to the number of parameters to estimate. Application of our approach allows one to compute reasonable estimates for most of the identifiable model parameters even when regression is inapplicable.

2 PARAMETER ESTIMATION IN LINLOG MODELS
The dynamics of metabolic networks are described by kinetic models having the form of systems of ordinary differential equations (ODEs) ():

where x‚àà‚Ñùn+ denotes the vector of (nonnegative) internal metabolite concentrations, u‚àà‚Ñùp+ the vector of external metabolite concentrations, e‚àà‚Ñùm+ the vector of enzyme concentrations, and v:‚Ñùn+p+m+‚Üí‚Ñùm the vector of reaction rate functions. N‚àà‚Ñ§n√óm is a stoichiometry matrix.

The reaction rates v are nonlinear and generally complex functions of x, u, and e, with many kinetic parameters that are difficult to reliably estimate from the data. This has motivated the use of approximate rate functions, like the linear-logarithmic (linlog) functions considered in this paper (; ). The linlog approximation expresses the reaction rates as proportional to the enzyme concentrations and to a linear function of the logarithms of internal and external metabolite concentrations. This leads to the rate equation

where the logarithm of a vector means the vector of logarithms of its elements. For conciseness, in the sequel we shall drop the dependence of v on (x,u,e) from the notation. An in-depth discussion of linlog models and comparison with other approximative rate functions can be found in the review by ).

We are interested in the estimation of the (generally unknown) parameters a‚àà‚Ñùm, Bx‚àà‚Ñùm√ón and Bu‚àà‚Ñùm√óp from q experimental datapoints (v(k),x(k),u(k),e(k)), k=1,‚Ä¶,q. That is, the data used for parameter estimation are parallel measurements of enzyme and metabolite levels as well as metabolic fluxes. The datapoints (v(k),x(k),u(k),e(k)) are obtained under different experimental conditions, for instance different dilution rates in continuous cultures or different mutant strains. Notice that in practice reaction rates are most of the time measured at (quasi-)steady state (see also ). That is, on the time-scale of interest the derivatives of metabolite concentrations vanish and Equation () can be rewritten as N¬∑v=0.

For the purpose of parameter estimation, it is convenient to rewrite () in the form of a regression model:

where the ratio of two vectors (here v/e) denotes elementwise division. Let us use an upperbar to denote the mean of a quantity over its q experimental observations, for instance: . By the linearity of (), it holds that

This allows () to be reformulated as a mean-removed model

and we obtain the following parameter estimation problem:

Problem 1.
Given the data matrices

find parameters C‚âú[Bx Bu]T solving the regression problem

where Œµ‚àà‚Ñùq√óm is measurement noise on W.

Notice that the parameter vector a no longer appears in the regression problem, but an estimate of it can be recovered from estimates of C=[Bx Bu]T by way of Equation ().

In the remainder of the article, we make the assumption that each column Œµ¬∑i of Œµ follows a Gaussian distribution, indicated by Œµ¬∑i~ùí©(0,Œ£Œµi), where Œ£Œµi is diagonal, i.e. the measurement errors in different experiments are mutually uncorrelated. We further assume that Œµ.i is independent of Œµ.j for i‚â†j. Then, Problem 1 can be subdivided into m independent subproblems, one for each reaction i:

where w¬∑i and c¬∑i are the ith columns of W and C, respectively.

The values of the parameter matrices Bx and Bu admit an interesting biological interpretation. Notice that one can immediately find values x0‚àà‚Ñùn+, u0‚àà‚Ñùp+, e0‚àà‚Ñùm+ and v0‚àà‚Ñùm such that , , and . As a consequence, Equation () can be rearranged into the common relative formulation of linlog models,

where 1m is an m√ó1 vector of ones, (v0,x0,u0,e0) is a so-called reference state () and Bx0, Bu0 are matrices of elasticity constants, where

The elasticities, introduced in the context of Metabolic Control Analysis (MCA) (), describe the normalized local response of the reaction rates to changes in metabolite concentrations. The interest is that they can thus be immediately computed from the values of Bx and Bu found by the solution of Problem 1, and the equality .

Although straightforward in theory, solving the regression problem () encounters two complications in practice.
Since the measurements are carried out at (quasi-)steady state, we have N¬∑v(x,u,e)=0. This introduces dependencies among the data and thus reduces the information content of the data matrix Y, in the sense that Y becomes rank deficient. Like in earlier work (), we use standard approaches to solve this problem. We notably rely on principal component analysis (PCA) (; ) applied to the data matrix Y to reduce the model order, i.e. the number of independent parameters, and ensure well-posedness of the regression problem (see Supplementary Section S1 for technical details). In summary, we use singular value decomposition (SVD), a technique decomposing the data matrix into dominant and marginal components according to a variance criterion. For the purpose of linear regression, this corresponds to decomposing the parameter vector into a reduced number of components that can be determined with certainty based on the data, while the remaining components are poorly determined, i.e. they are ‚Äònonidentifiable‚Äô, and are discarded with negligible effect on the fit. We note in passing that the columns of W and Y are zero-mean, an important requirement for the correctness of the outlined analysis.

The high-throughput datasets contain a substantial amount of missing values, due to experimental limitations or instrument failures. If, for any given reaction, we only used the datapoints in which all relevant metabolite concentrations, enzyme concentrations and metabolic fluxes playing a role in that reaction are available, then a large amount of data would have to be thrown away. In practice, we would run the risk that the parameters cannot be reliably identified. The development of a method that is capable of maximally exploiting the information contained in incomplete datasets for solving Problem 1 is the main subject of the article and will be fully developed in the later sections.




3 LIKELIHOOD-BASED IDENTIFICATION OF LINLOG MODELS FROM MISSING DATA
For every reaction i, we are concerned with the problem of estimating the unknown parameters c¬∑i of the model given in () in the case where some entries of Y are unknown. We address the estimation problem by a maximum-likelihood approach, which is known to yield optimal (unbiased and minimum variance) estimates for our problem setting in the case where Y is fully known. As the problem is identical for all reactions i, in the remainder of the section we will drop for simplicity index ¬∑i from the notation.

Let ‚Ñê be the set of indices (row, column) corresponding to the known entries of Y, i.e. (j,k)‚àà‚Ñê if and only if Yj,k is available. It is convenient to introduce the decomposition , where

Matrix  is fully determined: Once measurements  of  are collected, we treat  as fixed parameters of the regression problem. Matrix  collects the unknown entries of Y. We model these missing data as unobserved independent random variables, whose prior distributions encode our generic knowledge about them. Assuming that the a priori distributions are not known (worst case), we define a Gaussian prior for each quantity that is missing in an experiment based on the measurements of the same quantity available from other experiments. For every (j,k)‚àâ‚Ñê and ùí¥j,k={Yj‚Ä≤,k: (j‚Ä≤,k)‚àà‚Ñê} (assumed nonempty), we let


We can now formulate the estimation problem.

Problem 2.
Given measurements W=w and , compute the estimate ƒâ=arg maxc log ‚Ñí(c), with , where, for any c,  is the probability density function of W given  corresponding to model ()‚Äì().

Note that ‚Ñí(c) is a likelihood function for a linear model with missing data, in the sense that it is defined with respect to available data  only. One can express ‚Ñí(c) by marginalization,

where  is the standard likelihood function for model () given  and , with  varying over all possible values of , and  is determined by the prior (). The explicit solution to the integral is reported in Supplementary Section S2. A direct approach to solving Problem 2 is to maximize () by numerical optimization. However, the function is not convex in c, whence its direct optimization is prone to end up in local minima and the use of global optimization strategies is required.

Alternatively, we propose to tackle Problem 2 by an Expectation-Maximization (EM) algorithm (). EM provides a general methodology for the optimization of a likelihood function with missing information. It is based on an iterative two-step procedure that, for the problem at hand, we implement as follows. Let us define the random variable , so that model () becomes . Note that , where for any given c, mean and variance can be derived from (). Let ƒâ0 be an initial guess of c. At every iteration ‚Ñì=1,2,3,‚Ä¶, compute an updated estimate ƒâ‚Ñì from the estimate ƒâ‚Ñì‚àí1 at the previous iteration by performing the following EM steps:

Expectation: compute

Maximization: solve

In (),  is the joint probability density function of Z and W given  and c, while , w is the probability density function of Z given , W=w and ƒâ‚Ñì‚àí1. These quantities are easily expressed in terms of model () and the priors defined in () (see Supplementary Section B).

It can be proven that, at every iteration ‚Ñì, the EM algorithm increases the value of ‚Ñí(ƒâ‚Ñì, and eventually converges to a maximum of ‚Ñí (). While this is not necessarily a global maximum, EM has proven effective in many applications (; ). A key property is that convergence to a maximum is achieved even if () is not solved exactly: It suffices that ƒâ‚Ñì is such that Q(ƒâ‚Ñì|ƒâ‚Ñì‚àí1)‚â•Q(ƒâ‚Ñì‚àí1|ƒâ‚Ñì‚àí1), which is easily achieved even by a local optimization algorithm. In practice, we can use the explicit expression of ‚Ñí in Problem 2 for stopping the iterations, e.g. when the relative improvement on ‚Ñí falls below a specified threshold œÑ&gt;0:

To complete the implementation of the algorithm, one must express Q(c|ƒâ‚Ñì‚àí1) in a form convenient for maximization. As explained in Supplementary Section S2, one can express () as an explicit function of c for any given ƒâ‚Ñì‚àí1. In compact form:

where fc stands for a Gaussian distribution with variance   and mean , Œ∫fc is a function depending on c via Œºfc and Œ£fc, and the proportionality factor that we dropped (indicated by the presence of ‚àù in place of =) depends on ƒâ‚Ñì‚àí1 but not on c. Finally, KL(¬∑||¬∑) and H(¬∑) are the Kullback‚ÄìLeibler distance between distributions and the entropy of a distribution, respectively, for which, in the Gaussian case at hand, explicit formulas are available (; ). A slight technical complicacy is needed in case  is singular (see Supplementary Section S2 for all the mathematical details).

The availability of the closed-form expression () allows us to implement EM efficiently, i.e. with an explicit maximization problem that is solved numerically at all iterations. Once the parameter estimates are obtained, several methods from the literature can be used to assess the accuracy of the results by inferring confidence intervals. Examples are randomized methods such as bootstrapping () and the profile likelihood method by (). This method derives confidence intervals using a threshold on a function called the profile likelihood. In our application, this is obtained separately for each parameter cj by re-maximization of () with respect to all parameters ck‚â†j, for all values cj in a neighborhood of ƒâj.

4 VALIDATION ON SYNTHETIC DATA
Before applying the EM algorithm to actual biological identification problems, we test the performance of the method on simulated data. For this purpose, a synthetic model has been developed, a simplified variant of the linlog model of E.coli central metabolism studied in  below. The model, in the form (), contains 17 variables, representing internal and external metabolites involved in 25 reactions, and 78 parameters (see Supplementary Section S3 for the model equations). We generate data matrices Y from this model by means of simulation, for different percentages of missing data and experimental noise. Using the model structure and the simulated data, we solve Problem 1 for each reaction independently, as described in .

In order to assess the added value of our specific implementation of likelihood optimization, we first compare the performance of the EM algorithm of  with the direct maximization of the loglikelihood () implemented with a general-purpose Matlab optimization routine. This method will be referred to as MaxLL in the sequel.

Second, we compare the likelihood-based identification approaches with standard methods, notably linear regression (referred to as Rg) and the commonly-used multiple imputation (MI) method (; ). Regression is performed based on full datasets only, i.e. it does not consider an experimentally-determined datapoint (v(k),x(k),u(k),e(k)) when at least one of the measurements is missing. MI is based on imputation of missing data by random draws of the missing values, i.e. non-zero elements of , from the a priori distribution defined in (). Both methods thus exploit only part of the information contained in an incomplete dataset and provide a lower limit for quantifying the performance of the methods proposed in .

Third, we compare the results of EM with the least-squares identification of the model on complete datasets (a method referred to as RgF, where F stands for Full datasets). Though inapplicable to real data with missing measurements, the method is statistically optimal. Hence, it provides us an upper performance bound that can be used to assess the role of missing data in performance degradation, separately from the role of noise.

Most of the high-throughput datasets available in the literature have been obtained when metabolism is at (quasi-)steady-state (). In order to mimic available experimental data as closely as possible, simulated data obtained from the synthetic model should therefore be steady-state data. We generated steady states of ()‚Äì(), and recorded the corresponding metabolite concentrations and metabolic flux values for 30 different conditions, each consisting of a random change in the enzyme concentration with respect to a reference value.

We compared performance of the five methods described above (EM, MaxLL, MI, Rg, RgF) on datasets with different amounts of missing data (40% and 75%) for the metabolite concentrations and noise levels (10% and 20%) for w. The only difference with the dataset used for the reference method RgF is that the latter has no missing data. A noise level of 10% means that the distribution used to generate the noise has a standard deviation equal to 10% of the values in w. The percentages of missing data in the simulation study are comparable to those observed in practice [ and ()]. For every different combination of missing data percentage and noise level, a dataset was generated by homogeneously distributing missing data among columns of Y, the indices for each column being chosen at random. For every simulated scenario, randomly generated noise was added to w in the dataset.

For all of the above scenarios, identification of each reaction was addressed separately, in accordance with the discussion of . For every reaction, we first tested the identifiability of the synthetic linlog model by PCA of the full data matrix Y. In our simulation, 9 reactions out of the 25 composing the model were detected as having nonidentifiable parameters. For those reactions, identification of a reduced-order model

was performed in place of the identification of the original model. Y*‚àà‚Ñùq√ór, with r‚â§n+p, is a reduced-order data matrix obtained by linear transformation of Y, and c*‚àà‚Ñùr is a parameter vector, smaller than c, that is ‚Äòidentifiable‚Äô, in the sense that it is well determined by the data (see Supplementary Section S3).

We implemented the different parameter estimation algorithms in Matlab, using the lscov function for the regression-based methods and fminsearch for global optimization in MaxLL and the maximization step in EM. Both EM and MaxLL require an initial guess of the parameters to be specified. We proposed 10 different initial parameter vectors, including the estimation obtained with the baseline method Rg where available. In order to draw statistics for the estimation performance, each of the five algorithms was applied on 100 Monte-Carlo repetitions of the identification problem. The complete performance test over all methods, conditions and 100 repetitions took about 7 h 40 min in Matlab 7.4.0 on a Linux PC workstation (1862 MHZ, 2 GB RAM).

The most informative results from all identification methods are summarized by boxplots of the ratio of the estimated parameter values c over the reference parameter values cref used to simulate the data. The closer the ratio to 1, the better the estimates. Ensemble statistics are drawn for all parameters corresponding to the same reaction.  is dedicated to the scenario with 40% missing data and 10% noise, whereas  reports on 75% missing data and 20% noise. Complete results for all reactions under all conditions can be found in Supplementary Section S3.
Statistics of estimated parameter values for datasets with 40% of missing data and 10% noise. The results are shown as boxplots of the ratio of the estimated parameter values c and reference parameter values cref. Statistics have been computed for each of the 5 methods from 100 datasets. For each method, the red line displays the median and the lower and upper blue lines represent the lower and upper quartile values, respectively. Whiskers extend from each end of the box to the most extreme values within 1.5 times the interquartile range from the ends of the box and outliers are shown with red crosses. The tested algorithms are Expectation Maximization (EM), direct optimization of loglikelihood (MaxLL), multiple imputation (MI), regression on incomplete datasets (Rg) and regression on complete datasets (RgF). (A‚ÄìD) Boxplots for reactions 3, 4, 11 and 18 of the network, respectively.


Statistics of estimated parameter values for datasets with 75% of missing data and 20% noise. The graphical notations are the same as for . (A‚ÄìF) Boxplots for reactions 3, 13, 17, 22, 19 and 25 of the network, respectively.



Since the individual reactions of the model involve only a small subset of metabolites, each of the m identification subproblems consists of the estimation of a limited number of parameters, mostly 2 or 3. For the case with 40% missing data, Rg can therefore be performed in all runs for every reaction of the model. On the contrary, with 75% missing data, regression cannot be applied to 6 reactions which is apparent from the absence of the Rg statistics for 2 reactions in .

In comparison with the other methods, multiple imputation (MI) gives the worst results (largest bias) in 3 out of the 4 reactions shown in , and in 5 out of 6 reactions in . In reactions 11 of  and 22 of , the relatively small biases are accompanied by an estimation uncertainty wider than for EM and MaxLL. This could be explained by a restricted use of information contained in the distribution of missing data. Indeed, MI only considers random draws from the distribution while EM and MaxLL are based on all possible values taken by missing data through integration of the distribution.

Analysis of  reveals that, for 40% missing data and 10% noise, the performance of EM and MaxLL is almost identical and similar to that of regression (Rg and RgF), with limited improvements on Rg, i.e. slightly smaller variability. In some cases, such as for reactions 11 and 18, their performance approaches the optimal, unattainable bound provided by RgF, i.e. they have similar bias and variability.

Performance improvements of likelihood-based methods over Rg become more significant when identification is performed on the dataset with higher percentage of missing data and larger noise. A‚ÄìD show results for reactions where Rg was applicable. Both EM and MaxLL substantially reduce estimation variability in reactions 3, 17 and 22. At the same time, due to the larger amount of missing data, performance loss with respect to RgF is more significant. Turned another way, this shows the accuracy that could be recovered were all datasets complete.

E and F show the results when Rg fails to produce estimates and cannot be used to initialize EM and MaxLL optimization. Still, EM provides estimates of the right order of magnitude and, for the case of E, of the right sign in at least 75% of the runs (box entirely above 0), while the median has the right sign and is reasonably close to 1. The estimation of the sign provided by MaxLL is less reliable (box crossing 0).

Overall, we conclude that the EM-based approach provides the most accurate estimates under all simulated conditions. We will therefore apply this method to the identification of the linlog model of an actual metabolic network from a published high-throughput dataset.

5 Application to central metabolism in E.coli
The network of central carbon metabolism in E.coli has been studied for a long time from different perspectives, which makes it an ideal model system for our purpose. A rather precise idea of the structure of the network exists, several kinetic models of the network dynamics are available [(; ) and references therein], and recently a high-throughput dataset containing the required information for solving Problem 1 has been published (). The network we consider here gathers enzymes, metabolites and reactions that make up the bulk of E.coli central carbon metabolism, including glycolysis, the pentose-phosphate pathway, the tricarboxylic acid cycle and anaplerotic reactions such as glyoxylate shunt and PEP-carboxylase ().
Scheme of E.coli central carbon metabolism. This map, showing metabolites (bold fonts) and genes (italic) is adapted from (). Abbreviations of metabolites are glucose (Glc), glucose 6-phosphate (G6P), fructose 6-phosphate (F6P), fructose 1-6-biphosphate (FBP), dihydroxyacetone phosphate (DHAP), glyceraldehyde 3-phosphate (G3P), 3-phosphoglycerate (3PG), phosphoenolpyruvate (PEP), pyruvate (Pyr), 6-phosphogluconate (6PG), 2-keto-3-deoxy-6-phospho-gluconate (2KDPG), ribulose 5-phosphate (Ru5P), ribose 5-phosphate (R5P), xylulose 5-phosphate (X5P), sedoheptulose 7-phosphate (S7P), erythrose 4-phosphate (E4P), oxaloacetate (OAA), citrate (Cit), isocitrate (IsoCit), 2-keto-glutarate (2KG), succinate-CoA (SuccoA), succinate (Suc), fumarate (Fum), malate (Mal), glyoxylate (Glyox), acetyl-CoA (AcoA), acetylphosphate (Acp) and acetate (Ace). Cofactors impacting the reactions are not shown. The gene names are separated by a comma in the case of isoenzymes, by a colon for enzyme complexes, and by a semicolon when the enzymes catalyze reactions that have been lumped together in the model.



The dataset used for identification of this network was obtained by experiments with 24 single-gene disruptants that were grown at a fixed dilution rate of 0.2 h‚àí1 in a glucose-limited chemostat, and with wild-type cells at 5 different dilution rates (). The authors collected data using multiple high-throughput techniques, in particular DNA microarray analysis and 2D differential gel electrophoresis (2D-DIGE) for genes and proteins, capillary electrophoresis time-of-flight mass spectrometry (CE-TOFMS) for metabolites, and metabolic flux analysis. They thus obtained a dataset consisting of metabolite concentrations, mRNA and protein concentrations for the enzymes, and metabolic fluxes under 29 different experimental conditions. A large number of different metabolites were measured in the experiments, with missing data in varying amounts, from 0 to 80% of the observations, 28% on average for the metabolites considered below.

From the reactions listed in (), we have constructed a linlog model of the form (), with n=16 internal metabolites, p=7 external metabolites and measured cofactors, and m=31 reactions (see Supplementary Section S4). Each of the reactions is catalyzed by a single enzyme, which may actually stand for several enzymes in the case of isoenzymes, enzyme complexes or lumped reactions. Reactions have been simplified or lumped together when a shared metabolite has not been measured, which precludes estimation of the corresponding elements in the parameter matrices Bx and Bu. In comparison with an earlier linlog model of E.coli central carbon metabolism (), we extended the scope to include the tricarboxylic acid cycle and the glyoxylate shunt, but due to the above-mentioned simplifications our model is more coarse-grained.

An identifiability analysis was performed by several rounds of missing data imputation using the a priori distribution defined in Equation () and PCA, which led in each case to the same result: 7 out of 31 reactions were detected as having nonidentifiable parameters. For those reactions, the model has been reduced as described in Equation () using a data matrix Y completed by the means Œºj,k of the a priori distributions. For every individual reaction, the reduced model has a parameter vector c* that is now entirely identifiable.

Apart from the distribution of the a priori missing data, given by Equation (), application of EM requires information about the distribution of Œµ, the error on the ratios of fluxes and enzyme concentrations. The Ishii dataset provides several replica measurements for a reference experimental condition: wild-type cells grown in a glucose-limited chemostat with a dilution rate of 0.2 h‚àí1. These data were used for the computation of the variance of Œµ. In order to assess the accuracy of the estimated Bx and Bu, we computed for each parameter a 95% confidence interval, by means of the profile likelihood method outlined in . Running the EM method on the model and the data took about 220 s using the implementation of . The computation of the confidence intervals for all parameters required about 23 min.

Contrary to the simulation studies reported in , a reference or ‚Äòreal‚Äô model for the evaluation of the results does not exist in this case. However, a priori biochemical knowledge on the signs of the elasticities is available, i.e. elasticities are positive for substrates and negative for products. This information can be compared with the estimated signs of the elasticities, and their confidence intervals, computed from the parameter matrices using the relations in Equation (). The results are shown in . Similar unshown results are obtained by means of the MaxLL method.
Elasticity matrix [B0x B0u] estimated by EM from the data of () for the linlog model of E.coli central carbon metabolism (the columns of the matrix have been permuted for readability)

	
Unidentifiable elasticities are shown in grey, uncertain elasticities (i.e. having a sign that is not significant with 95% confidence) in yellow, and correctly/incorrectly identified elasticities (i.e. having a sign that is significant with 95% confidence) in green/red. Abbreviations are as in . Some of the cofactors are modeled as ratios of metabolite concentrations, e.g. ATP/ADP. Reaction 27, labeled Œº, is a phenomenological reaction for biomass production. The last row indicates the percentage of missing data per metabolite and the right-most column displays the amount of complete datapoints available for each reaction. aRegression was not able to produce any result.




We observe that the EM method obtains estimates for all reactions, including the 7 cases where the insufficient amount of data made regression not applicable. However, 26 of the 100 non-zero elasticities of the model are not identifiable from this dataset. Moreover, out of the remaining 74 elasticity estimates, more than half of them have signs that are not statistically significant, in the sense that the 95% confidence interval straddles 0. This is most likely due to the fact that the magnitude of noise in metabolite concentrations is comparable to the magnitude of relevant information. For example, for PEP the standard deviation over all experimental conditions equals the standard deviation of the replicates in a single condition (0.06 mM versus 0.05 mM). This precludes the estimation of an unambiguous sign.

Of the elasticities with statistically significant signs, 20 out of 34 are correct, in the sense that they have the expected positive or negative sign. The remaining elasticities, distributed over 9 reactions, are incorrectly estimated. Let us now discuss what we believe are potential sources of these errors, giving information that could be used to single out erroneous estimates a priori.

We first note that for 3 of these 9 reactions (GapA;Pgk, Mdh and Edd;Eda, see ), only very few complete datapoints are available (between 3 and 5) and regression mostly fails in these cases. In addition, all of these reactions involve at least one metabolite missing in &gt;70% of the experimental conditions. The combination of very few complete datapoints and a high percentage of missing metabolite measurements obviously makes model identification extremely difficult and it is fair to say that here we reach the limit of the applicability of our method, or of any method for that matter, due to the lack of data.

Second, 4 reactions are known to operate close to equilibrium: Pgi, FbaA,FbaB, TpiA and GpmA,GpmB;Eno (). Theoretically, these reactions are not identifiable, as their elasticities are not independent (), but PCA did not detect this. Most likely, this is due to the above-mentioned noise in metabolite concentrations, which decreases their correlations. A cautious, preemptive strategy would be to reduce the model for any reaction known to be close to equilibrium and eliminate the corresponding dependent variables.

The errors in the signs of some elasticities in the remaining 2 reactions (PtsG and PykA,PykF) are less straightforward to explain. It is unlikely that they can be attributed to the EM method, given that regression is applicable here with a relatively large number of complete datapoints available (14 and 11, respectively) and gives the same results. Alternatively, they may be explained by a modeling error or a hidden variable, for instance an unknown cofactor, biasing the estimation results. It is also possible that the approximations of the linlog model are not suitable for these reactions, for instance because there are large variations in metabolite concentrations between conditions, driving the system far from the reference state.

In summary, EM gives reasonable results for a fairly complicated model on a challenging dataset. Even though some puzzling issues remain, we believe that these can be safely attributed to the inherent difficulty of the identification problem.

6 DISCUSSION
In this work, we have addressed the problem of estimating parameters of approximate models of metabolic networks from incomplete datasets. Even with the largest datasets available at present, such as those reported in (), the absence or corruption of a large number of measurements may reduce the effective number of datapoints to a handful of experimental conditions, thus making simple regression techniques ineffective or even inapplicable. Making full use of all the available data is therefore essential to render identification well-posed and improve the quality of the estimated models.

To this aim, we have proposed a maximum-likelihood method for the identification of linlog metabolic network models that compensates for the missing data by the use of statistical priors. We developed an algorithm that attains maximization of the likelihood based on Expectation Maximization, a well accepted paradigm for the numerical optimization of likehood functions in the presence of unobserved variables. A simpler implementation based on direct likelihood maximization via general-purpose numerical optimization algorithms was also considered and found slightly less powerful. The performance of EM was compared to that of an existing method of reference, namely multiple imputation, and to worst-case and best-case scenarios given by least-squares regression on the sole complete datapoints and on complete datasets, respectively. We showed that EM outperforms multiple imputation by a wide margin. In comparison with worst-case regression, it reduces the estimation variability and is able to produce reasonable estimation results even when regression on incomplete datasets is inapplicable. It also approaches the ideal performance of regression on complete datasets for low rates of missing data, regardless of noise.

Based on these findings, we applied EM to the identification of a linlog model for the central carbon metabolism in the bacterium E.coli from the experimental data presented by ). Even with the large amount of incomplete datapoints, due to the difficulty of experimentally measuring metabolite concentrations, EM was able to estimate many of the model parameters (elasticities) in agreement with the current understanding of the system. This is even true for reactions where the reduced number of complete datapoints impairs the applicability of least squares regression. On the other hand, the challenging quality of the data sheds light on the performance limits of the method, which tends to fail when large measurement noise makes the estimation of small parameters statistically unreliable, when the same variable cannot be measured in most conditions, or when reactions operate near equilibrium.

Overall, results from the simulations and the application on real data showed that our EM approach is able to make the most of incomplete, noisy high-throughput datasets for the estimation of parameters in approximate kinetic models. In the future, we expect to improve performance by developing a number of technical points, including approximate analytical/dedicated numerical solutions for the EM maximization steps, the refinement of the identifiability analysis via SVD of incomplete data matrices (), and a more detailed modeling of measurement noise. It is worth noting that, while the method has been developed for linlog models, it is more generally applicable to any other metabolic network model that can be put in a form linear in the parameters by straightforward manipulations, such as generalized mass action models that provide advantages when some metabolite concentrations approach 0 (; ). In addition, estimated parameters of approximate metabolic models, such as elasticities of linlog models, provide useful hints for the identification of more detailed nonlinear kinetic models.

From a broader perspective, the application of the EM method to a unique multi-omics dataset for E.coli carbon metabolism allowed us to isolate issues that are critical for the appropriate exploitation of the data for parameter estimation. These issues may need to be taken into account during the design of the experiments. One such issue is that a high percentage of missing data for some of the individual variables, even at a relatively low average percentage over the entire dataset, was found to be much detrimental to the identification results. This may influence sampling strategies, especially for metabolites that are difficult to measure. Another issue is the identifiability problems caused by steady-state measurements, which cannot always be resolved by genetic mutation or by varying physiological conditions. From this perspective time-resolved observations of the network dynamics, although much more demanding experimentally, carry great promise ().

Supplementary Material
Supplementary Data
ACKNOWLEDGEMENTS
The authors would like to thank Delphine Ropers for useful discussions.

Funding: Agence Nationale de la Recherche under project MetaGenoReg (ANR-06-BYOS-0003).

Conflict of Interest: none declared.

